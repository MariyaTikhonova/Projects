{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Языковое моделирование\n",
    "Курс NLP, лабораторная 1.  \n",
    "Осень 2015.\n",
    "\n",
    "В данном задании вы реализуете:\n",
    "\n",
    "- Add-one smoothing\n",
    "- Stupid backoff\n",
    "- Interpolation smoothing\n",
    "- EM-algorithm\n",
    "- Kneser-Ney smoothing\n",
    "\n",
    "Вы примените это к:\n",
    "\n",
    "- Language recognition problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подпись**: Тихонова Мария Ивановна - 2 курс - КН"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель языкового моделирования заключается в том, чтобы присвоить некоторые вероятности предложениям. Возникает закономерный вопрос, а зачем нам это надо? Например, в задачах _машинного перевода_ частенько нужно среди нескольких предложений выбирать наиболее вероятный перевод (который является естественным для человеческого глаза). Также это чрезвычайно полезно в задаче _исправления опечаток_ и _распозновании речи_.\n",
    "\n",
    "Наша задача состоит в подсчете вероятности $P(W) = P(w_1, \\dots, w_n)$ или $P(w_n \\mid w_1, \\dots, w_{n-1})$. Модель, умеющая вычислять хотя бы одну из этих двух вероятностей, называется **язковой моделью** (LM от Language Model).\n",
    "\n",
    "Пришло время вспомнить _цепное правило_ (chain rule) $P(X_1, \\dots, X_n) = P(X_1)P(X_2 \\mid X_1)\\dots P(X_n \\mid X_1, \\dots, X_{n-1})$. Также мы знаем, что\n",
    "\n",
    "$$\n",
    "    P(X_n \\mid X_1, \\dots, X_{n-1}) = \\frac{P(X_1, \\dots, X_n)}{P(X_1, \\dots, X_{n-1})},\n",
    "$$\n",
    "\n",
    "следовательно, для того чтобы оценить $P(X_n \\mid X_1, \\dots, X_{n-1})$ нужно посчитать $P(X_1, \\dots, X_n)$ и $P(X_1, \\dots, X_{n-1})$. Но эти вероятности будут чрезвычайно малы, если мы возьмем большое $n$, так множество предложений из $n$ слов растет экспоненциально. Для упрощения ситуации применим **марковское предположение**: $P(X_n \\mid X_1, \\dots, X_{n-1}) = P(X_n \\mid X_{n - k + 1}, \\dots, X_{n-1})$ для некоторого фиксированного (небольшого) $k$. Это предположение интуитивно ясно и говорит нам о том, что $X_{n}$ не зависит от $X_{1}, \\dots, X_{n - k}$, то есть на следующее слово влияет лишь контекст из предыдущих $k - 1$ слова. Таким образом, мы получаем финальную вероятность:\n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_i P(w_i \\mid w_{i-k+1}, \\dots, w_{i - 1}).\n",
    "$$\n",
    "\n",
    "Далее для краткости будем обозначать $w_{i-k}^i := w_{i-k}, \\dots, w_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хранилище n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пришло время написать класс для хранения n-грамм. Следуйте комментариям, чтобы написать NGramStorage с удобным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPS = 10 ** (-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramStorage:\n",
    "    \"\"\"Storage for ngrams' frequencies.\n",
    "    \n",
    "    Args:\n",
    "        sents (list[list[str]]): List of sentences from which ngram\n",
    "            frequencies are extracted.\n",
    "        max_n (int): Upper bound of the length of ngrams.\n",
    "            For instance if max_n = 2, then storage will store\n",
    "            0, 1, 2-grams.\n",
    "            \n",
    "    Attributes:\n",
    "        max_n (Readonly(int)): Upper bound of the length of ngrams.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, sents=[], max_n=0):\n",
    "        self.__max_n = max_n\n",
    "        self.__ngrams = {i: Counter() for i in range(self.__max_n + 1)}\n",
    "        \n",
    "        # self._ngrams[K] should have the following interface:\n",
    "        # self._ngrams[K][(w_1, ..., w_K)] = number of times w_1, ..., w_K occured in words\n",
    "        # self._ngrams[0][()] = number of all words\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        for i in range(1, self.__max_n + 1):\n",
    "            for sent in sents:\n",
    "                for ngram in ngrams(sent, i):\n",
    "                    self.__ngrams[i][ngram] += 1\n",
    "        self.__ngrams[0][()] = sum(self.__ngrams[1].values())\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        \"\"\"Add UNK token to 1-grams.\"\"\"\n",
    "        # In order to avoid zero probabilites \n",
    "        if self.__max_n == 0 or u'UNK' in self.__ngrams[1]:\n",
    "            return\n",
    "        self.__ngrams[0][()] += 1\n",
    "        self.__ngrams[1][(u'UNK',)] = 1\n",
    "        \n",
    "    @property\n",
    "    def max_n(self):\n",
    "        \"\"\"Get max_n\"\"\"\n",
    "        return self.__max_n\n",
    "        \n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Get dictionary of k-gram frequencies.\n",
    "        \n",
    "        Args:\n",
    "            k (int): length of returning ngrams' frequencies.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary (in fact Counter) of k-gram frequencies.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(k, int):\n",
    "            raise TypeError('k (length of ngrams) must be an integer!')\n",
    "        if k > self.__max_n:\n",
    "            raise ValueError('k (length of ngrams) must be less or equal to the maximal length!')\n",
    "        return self.__ngrams[k]\n",
    "    \n",
    "    def __call__(self, ngram):\n",
    "        \"\"\"Return frequency of a given ngram.\n",
    "        \n",
    "        Args:\n",
    "            ngram (tuple): ngram for which frequency should be computed.\n",
    "            \n",
    "        Returns:\n",
    "            Frequency (int) of a given ngram.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(ngram, tuple):\n",
    "            raise TypeError('ngram must be a tuple!')\n",
    "        if len(ngram) > self.__max_n:\n",
    "            raise ValueError('length of ngram must be less or equal to the maximal length!')\n",
    "        if len(ngram) == 1 and ngram not in self.__ngrams[1]:\n",
    "            return self.__ngrams[1][(u'UNK', )]\n",
    "        return self.__ngrams[len(ngram)][ngram]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте скачаем корпус и запустим на нем наши эксперименты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Uncomment next row and download brown corpus\n",
    "# nltk.download()\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences = 57340\n",
      "Number of train sentences = 45872\n",
      "Number of test sentences = 11468\n"
     ]
    }
   ],
   "source": [
    "all_sents = list(brown.sents())\n",
    "random.seed(123)\n",
    "random.shuffle(all_sents)\n",
    "print('Number of all sentences = {}'.format(len(all_sents)))\n",
    "train_sents = all_sents[:int(0.8 * len(all_sents))]\n",
    "test_sents = all_sents[int(0.8 * len(all_sents)):]\n",
    "print('Number of train sentences = {}'.format(len(train_sents)))\n",
    "print('Number of test sentences = {}'.format(len(test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create storage of 0, 1, 2, 3-grams\n",
    "storage = NGramStorage(train_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371\n",
      "3265\n",
      "28\n",
      "0\n",
      "930601\n"
     ]
    }
   ],
   "source": [
    "# It's time to test your code\n",
    "print(storage(('to', 'be')))\n",
    "print(storage(('or',)))\n",
    "print(storage(('not', 'to', 'be')))\n",
    "print(storage(('Muammar',)))\n",
    "print(storage(()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим **перплексию**:\n",
    "\n",
    "$$\n",
    "    {\\rm PP}(w_1, \\dots, w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_i P(w_i \\mid w_{i - k}, \\dots, w_{i - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "кстати, из этой формулы видно, что задача по минимизации перплексии равносильна задаче по максимизации вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию по подсчету перплексии. Стоит отметить, что перплексия по корпусу $-$ это не то же самое, что и усредненная перплексия по предложениям. Перплексия по корпусу равна произведению вероятностей **всех** предложений в степени $-\\frac1N$, где $N -$ суммарная длина всех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(estimator, sents):\n",
    "    '''Estimate perplexity of the sequence of words using prob_estimator.'''\n",
    "    ### YOUR CODE HERE\n",
    "    # Avoid log(0) by replacing zero by 10 ** (-50).\n",
    "    perp = 0.\n",
    "    L = 0.\n",
    "    for sent in sents:\n",
    "        L += len(sent)\n",
    "        prob = estimator.prob(sent)\n",
    "        perp += math.log(prob + 10 ** (-50) * float(prob < 10 ** (-50)))\n",
    "    # deg = sum(len(sent) for sent in sents)\n",
    "    perp = math.exp(-1. / L * perp)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Оценка вероятностей n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый и простейший способ оценки вероятностей N-грам следующий:\n",
    "\n",
    "$$\n",
    "    \\hat P_{S}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N)}{c(w_1^{N-1})}.\n",
    "$$\n",
    "\n",
    "где $c(w_1^N)$ — это число последовательностей $w_1, \\dots, w_N$ в корпусе, $S$ символизирует Straightforward. Что-ж, пора это реализовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StraightforwardProbabilityEstimator:\n",
    "    \"\"\"Class for simplest probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = c(context + word) / c(context), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage):\n",
    "        self.__storage = storage\n",
    "        # Adding UNK token to avoid zero probabilities\n",
    "        self.__storage.add_unk_token()\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if self.__storage.max_n == 1:\n",
    "            return ()\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a string!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        # Avoiding 0 / 0.\n",
    "        if context_counts == 0:\n",
    "            return 0.\n",
    "        return 1. * phrase_counts / context_counts\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 265.54563079779234\n",
      "1.9342318198327532e-05\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "simple_estimator = StraightforwardProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(simple_estimator, test_sents)))\n",
    "print(simple_estimator.prob('To be'.split()))\n",
    "print(simple_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем перплексию униграмной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 108.52877490449828\n"
     ]
    }
   ],
   "source": [
    "uni_storage = NGramStorage(train_sents, 1)\n",
    "uni_simple_estimator = StraightforwardProbabilityEstimator(uni_storage)\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(uni_simple_estimator, test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<font color='red'>Ответьте на следующие вопросы (внутри ipython ноутбука):</font>\n",
    "\n",
    "**Q:** Какие выводы можно сделать? Почему $P(\\text{To be or not to be}) = 0$, хотя мы и добавили UNK токен?  \n",
    "**A:**\n",
    "\n",
    "**Q:** Почему перплексия униграмной модели меньше, чем триграмной?  \n",
    "**A:**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы ответить на вопросы посмотрим, какие верятности у всех ngram из нашего предложения 'To be or not to be'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word To\n",
      "Context ()\n",
      "Probability for trigrams prediction 0.0003707277654679444\n",
      "Probability for unigrams prediction 0.0003707277654679444\n",
      "\n",
      "\n",
      "Word be\n",
      "Context ('To',)\n",
      "Probability for trigrams prediction 0.05217391304347826\n",
      "Probability for unigrams prediction 0.005476025196593173\n",
      "\n",
      "\n",
      "Word or\n",
      "Context ('To', 'be')\n",
      "Probability for trigrams prediction 0.0\n",
      "Probability for unigrams prediction 0.003508481606529967\n",
      "\n",
      "\n",
      "Word not\n",
      "Context ('To', 'be', 'or')\n",
      "Probability for trigrams prediction 0.0\n",
      "Probability for unigrams prediction 0.003790019793638956\n",
      "\n",
      "\n",
      "Word to\n",
      "Context ('To', 'be', 'or', 'not')\n",
      "Probability for trigrams prediction 0.041666666666666664\n",
      "Probability for unigrams prediction 0.02212976116535318\n",
      "\n",
      "\n",
      "Word be\n",
      "Context ('To', 'be', 'or', 'not', 'to')\n",
      "Probability for trigrams prediction 0.20437956204379562\n",
      "Probability for unigrams prediction 0.005476025196593173\n",
      "\n",
      "\n",
      "Total probability for trigram model 0.0\n",
      "Total probability for unigram model 5.255461077476345e-103\n"
     ]
    }
   ],
   "source": [
    "sent = 'To be or not to be'\n",
    "contex = tuple('To be or not to be'.split())\n",
    "for i in range(len(contex)):\n",
    "    print('Word {}'.format(contex[i]))\n",
    "    print('Context {}'.format(contex[:i]))\n",
    "    print('Probability for trigrams prediction {}'.format(simple_estimator(contex[i], contex[:i])))\n",
    "    print('Probability for unigrams prediction {}'.format(uni_simple_estimator(contex[i], contex[:i])))\n",
    "    print('\\n')\n",
    "\n",
    "print('Total probability for trigram model {}'.format(simple_estimator.prob(sent)))\n",
    "print('Total probability for unigram model {}'.format(uni_simple_estimator.prob(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь все понятно. Из-за того, что вероятность некоторых слов в данном контексте в исходном предложении равна 0, то и вероятность всего предложения, которая считается как произведение вероятностей, тоже равна нулю. Вывод:  у StraightforwardProbabilityEstimator есть один большой недостаток: если вероятность хотя бы какого-то слова в данном контексте равна нулю, то и вероятность всего предложения окажется нулевой, что не обязательно верно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У униграммной модели этого недостатка нет, поэтому нулевых вероятностей мы не получаем (как видно из примера, вероятность модельного предложения для униграмной модели ненулевая), поэтому и перплексия меньше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-one smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейший вид сглаживания — **сглаживание Лапласа**. Каким-то образом мы хотим избавиться от нулевых вероятностей. Наиболее простой алгоритм борьбы с этой проблемой следующий: для оценки $P(w_{N} \\mid w_1^{N - 1})$ мы будем использовать формулу:\n",
    "\n",
    "$$\n",
    "    \\hat P_{AOS}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N) + \\delta}{c(w_1^{N-1}) + \\delta V},\n",
    "$$\n",
    "\n",
    "где $V$ — это размер словаря, а $\\delta$ — некоторая фиксированная константа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пришло время снова пописать код. Реализуйте класс, симулирующий сглаживание Лапласа. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LaplaceProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = (c(context + word) + delta) / (c(context) + delta * V), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus,\n",
    "    delta - some constant,\n",
    "    V - number of different words in corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): Smoothing parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        # Adding UNK token to avoid zero probabilities\n",
    "        # self.__storage.add_unk_token()\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('context must be a tuple!')\n",
    "            \n",
    "        ### YOUR CODE HERE\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        # Avoiding 0 / 0.\n",
    "        prob = 1.\n",
    "        # if context_counts == 0: Эта строчка здесь не нужна, так как идея сноаживания как раз в том\n",
    "        #чтобы уйти от нулевых вероятностей\n",
    "            # return 0.\n",
    "        return 1. * (phrase_counts + self.__delta)/ (context_counts +  self.__delta * len(self.__storage[1]))\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        # return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите наилучший параметр $\\delta$ для данного корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def laplas_perplexity(x):\n",
    "    return perplexity(LaplaceProbabilityEstimator(storage, abs(x)), test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best delta equals = 0.0003861116037380595\n",
      "Laplace estimator perplexity = 137.28000600290179\n",
      "1.83054888539e-05\n"
     ]
    }
   ],
   "source": [
    "best_delta = 1.\n",
    "best_delta = minimize_scalar( lambda x: laplas_perplexity(x), bounds=(EPS, 1.0), method='bounded', ).x\n",
    "\n",
    "print('Best delta equals = {}'.format(best_delta))\n",
    "laplace_estimator = LaplaceProbabilityEstimator(storage, best_delta)\n",
    "\n",
    "print('Laplace estimator perplexity = {}'.format(perplexity(laplace_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stupid backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **тупого откатывания** невероятно проста (на то оно и тупое!). Если у нас есть достаточно информцаии для подсчета вероятности $k$-грам, тогда будем использовать $k$-грамы. Если эта вероятность крайне мала, то будем использовать вероятности $(k-1)$-грам с некоторым множителем, например, $0.4$, и так далее. К сожалению, в данном случае мы получим не вероятностное распределение, но в большинстве задач это не принципиально важно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс, симулирующий сглаживание тупым откатыванием. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StupidBackoffProbabilityEstimator:\n",
    "    \"\"\"Class for stupid backoff probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        P'(word | context),                  if  P'(word | context) > 0;\n",
    "        P'(word | context[1:]) * multiplier, if  P'(word | context) == 0\n",
    "                                             and P'(word | context[1:]) > 0;\n",
    "        ...\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        multiplier (float): Multiplier which is used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, multiplier=0.1):\n",
    "        self.__base_estimator = base_estimator\n",
    "        self.__mult = multiplier\n",
    "        \n",
    "    def __backoff__(self, word, context):\n",
    "        original_prob = self.__base_estimator(word, context)\n",
    "        if original_prob > EPS:\n",
    "            return original_prob\n",
    "        else:\n",
    "            return self.__mult * self.__backoff__(word, context[1:])\n",
    "    \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        prob = self.__backoff__(word, context)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stupid backoff estimator perplexity = 123.53797910506032\n",
      "1.9342318198327532e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "sbackoff_estimator = StupidBackoffProbabilityEstimator(simple_estimator, .4)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Stupid backoff estimator perplexity = {}'.format(perplexity(sbackoff_estimator, test_sents)))\n",
    "print(sbackoff_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<font color='red'>Ответьте на следующие вопросы (внутри ipython ноутбука):</font>\n",
    "\n",
    "**Q:** Почему бессмысленно измерять перплексию в случае **Stupid backoff**?  \n",
    "**A:**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перплексию в данном случае считать бессмысленно, так как в случае Stupid backoff полученное распределение не вероятностное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае идея сглаживания посредством **интерполяции** также крайне проста. Пусть у нас есть $N$-грамная модель. Заведем вектор $\\bar\\lambda = (\\lambda_1, \\dots, \\lambda_N)$, такой, что $\\sum_i\\lambda_i = 1$ и $\\lambda_i \\geq 0$. Ну а далее просто постулируем, что\n",
    "\n",
    "$$\n",
    "    \\hat P_{IS}(w_{N} \\mid w_1^{N-1}) = \\sum_{i=1}^N \\lambda_i \\hat P_{S}(w_N \\mid w_{N-i+1}^{N-1}).\n",
    "$$\n",
    "\n",
    "Придумайте, как обойтись одним вектором $\\bar\\lambda$. Казалось бы, их нужно несколько, ибо наша модель должна уметь считать вероятности $P(w_3 \\mid w_1, w_2)$, а иногда $P(w_2 \\mid w_1)$. Если мы тупо обрубим сумму, то у нас уже не будет вероятностное распределение, что, конечно же, плохо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InterpolationProbabilityEstimator:\n",
    "    \"\"\"Class for interpolation probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        lambda_N * P'(word | context) +\n",
    "        lambda_{N-1} * P'(word | context[1:]) +\n",
    "        ... +\n",
    "        lambda_1 * P'(word)\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        lambdas (np.array[float]): Lambdas which are used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, lambdas):\n",
    "        self.lambdas = lambdas\n",
    "        self.__base_estimator = base_estimator\n",
    "        \n",
    "    def __normilize_prob__(self, iterations, prob):\n",
    "        sum = 0.0\n",
    "        for i in range(iterations):\n",
    "            sum += self.lambdas[i]\n",
    "        return prob/sum\n",
    "            \n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        prob = 0.\n",
    "        iterations = min(len(context)+1, len(self.lambdas))\n",
    "        for i in range(iterations):\n",
    "            prob += self.lambdas[i] * self.__base_estimator(word, context[len(context)-i:])\n",
    "        #print(prob)\n",
    "        prob = self.__normilize_prob__(iterations, prob)\n",
    "        #print(prob)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation estimator perplexity = 92.38161856595359\n",
      "Probability of TO BE:\n",
      "1.06862163916e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "interpol_estimator = InterpolationProbabilityEstimator(simple_estimator, np.array([0.2, 0.2, 0.6]))\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Interpolation estimator perplexity = {}'.format(perplexity(interpol_estimator, test_sents)))\n",
    "#print(laplace_estimator.prob('To be'.split()))\n",
    "print('Probability of TO BE:')\n",
    "print(interpol_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остается один вопрос: как подбирать $\\bar\\lambda$? Для этого обычно применяется EM-алгоритм. Сразу опишем лишь готовую формулу. Пусть $\\bar\\lambda^t$ соответствует набору коэффициентов на $t$-м шаге. $\\bar\\lambda^0$ задаем произвольно.\n",
    "\n",
    "* **E-step**:\n",
    "$$\n",
    "    \\hat\\lambda_j^t = \\sum_{w_1^N} \\frac{\\lambda_j^t \\cdot \\hat P_{S}(w_N \\mid w_{N-j+1}^{N-1})}{\\sum_{i=1}^N \\lambda_i^t \\hat P_{S}(w_N \\mid w_{N-i+1}^{N-1})}.\n",
    "$$\n",
    "* **M-step**:\n",
    "$$\n",
    "    \\lambda_j^{t+1} = \\frac{\\hat\\lambda_j^t}{\\sum_{i=1}^N \\hat\\lambda_i^t}.\n",
    "$$\n",
    "\n",
    "Формулы выписаны, то бишь самое сложное сделано. Вам остается это лишь реализовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def E_step(test_storage, s_estimator, i_estimator):\n",
    "    ### YOUR CODE HERE\n",
    "    estimated_lambdas = np.zeros(test_storage.max_n)\n",
    "    \n",
    "    for ngam in test_storage[test_storage.max_n].keys():\n",
    "        znam = sum(i_estimator.lambdas[i] * s_estimator(str(ngam[-1]), ngam[-i - 1:-1]) for i in range(test_storage.max_n))\n",
    "        for j in range(test_storage.max_n):\n",
    "            chisl = i_estimator.lambdas[j] * s_estimator(str(ngam[-1]), ngam[-j - 1:-1])\n",
    "            estimated_lambdas[j] += chisl / znam\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return estimated_lambdas\n",
    "\n",
    "def M_step(estimated_lambdas):\n",
    "    new_lambdas = estimated_lambdas\n",
    "    sum = np.sum(estimated_lambdas)\n",
    "    for j in range(len(new_lambdas)):\n",
    "        new_lambdas[j] /= sum\n",
    "    return new_lambdas\n",
    "\n",
    "def EM_algorithm(test_storage, s_estimator, i_estimator, epsilon=0.03):\n",
    "    ### YOUR CODE HERE\n",
    "    while True:\n",
    "        old_lambdas = i_estimator.lambdas[:]\n",
    "        estimated_lambdas = E_step(test_storage, s_estimator, i_estimator)\n",
    "        i_estimator.lambdas = M_step(estimated_lambdas)\n",
    "        norm = LA.norm(i_estimator.lambdas - old_lambdas)\n",
    "        print(norm)\n",
    "        if norm < epsilon:\n",
    "            break\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return i_estimator.lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate train into train and test\n",
    "train_set = train_sents[:int(0.6 * len(train_sents))]\n",
    "test_set = train_sents[int(0.6 * len(train_sents)):]\n",
    "MAX_N = 3\n",
    "train_storage = NGramStorage(train_set, MAX_N)\n",
    "test_storage = NGramStorage(test_set, MAX_N)\n",
    "s_estimator = StraightforwardProbabilityEstimator(train_storage)\n",
    "\n",
    "# Make starting assumption\n",
    "starting_lambdas = np.array([0.33, 0.33, 0.34])\n",
    "i_estimator = InterpolationProbabilityEstimator(s_estimator, starting_lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.324386547703\n",
      "0.0489980424294\n",
      "0.01455548097\n"
     ]
    }
   ],
   "source": [
    "# It can take some time\n",
    "best_lambdas = EM_algorithm(test_storage, s_estimator, i_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation estimator perplexity = 87.33784337510922\n",
      "9.07135205045e-06\n",
      "Best lambdas = [ 0.56881651  0.38995087  0.04123262]\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "i_estimator = InterpolationProbabilityEstimator(simple_estimator, best_lambdas)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Interpolation estimator perplexity = {}'.format(perplexity(i_estimator, test_sents)))\n",
    "print(i_estimator.prob('To be'.split()))\n",
    "print('Best lambdas = {}'.format(best_lambdas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея данного сглаживания заключается в том, что словам, которые участвуют в большом количестве контекстов, присваиваются большие вероятности, а те, которые исползуются в паре-тройке контекстов получают маленькие вероятности. Авторы данного сглаживания формализовали это следующим образом. Введем обозначения\n",
    "\n",
    "$$\n",
    "    N_{1+}(\\cdot w_2) := \\left|\\{w_1 : c(w_1, w_2) > 0\\}\\right|,\n",
    "$$\n",
    "$$\n",
    "    N_{1+}(\\cdot \\cdot) := \\sum_{w_2} N_{1+}(\\cdot w_2),\n",
    "$$\n",
    "$$\n",
    "    \\hat P_{KN} (w_1) = \\frac{{\\rm max}\\{N_{1+}(\\cdot w_1)-\\delta,0\\}}{N_{1+}(\\cdot \\cdot)} + \\frac{\\delta}{|V|}.\n",
    "$$\n",
    "\n",
    "Далее мы используем реккурентное соотношение\n",
    "\n",
    "$$\n",
    "    \\hat P_{KN}(w_{N} \\mid w_1^{N-1}) = \\frac{{\\rm max}\\{c(w_1^N) - \\delta, 0\\}}{\\sum_{w_N}c(w_1^{N-1}w_N)} + \\frac{\\delta}{\\sum_{w_N}c(w_1^{N-1}w_N)}N_{1+}(w_1^{N-1}\\cdot)\\hat P_{KN}(w_N \\mid w_2^{N-1}).\n",
    "$$\n",
    "\n",
    "Для вас дело за малым — реализовать это."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# При написании кода я пользовалась следующей, исправленной формулой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Идея данного сглаживания заключается в том, что словам, которые участвуют в большом количестве контекстов, присваиваются большие вероятности, а те, которые исползуются в паре-тройке контекстов получают маленькие вероятности. Авторы данного сглаживания формализовали это следующим образом. Введем обозначения\n",
    "\n",
    "$$\n",
    " N_{1+}(\\cdot w_2) := \\left|\\{w_1 : c(w_1, w_2) > 0\\}\\right|,\n",
    "$$\n",
    "$$\n",
    " N_{1+}(\\cdot \\cdot) := \\sum_{w_2} N_{1+}(\\cdot w_2),\n",
    "$$\n",
    "На семинаре я показывал, что надо изменить формулу:\n",
    "$$\n",
    " \\hat P_{KN} (w_1) = \\frac{\\max(N_{1+}(\\cdot w_1) - \\delta, 0)}{N_{1+}(\\cdot \\cdot)} + \\frac{\\delta \\|\\left\\{ w \\colon N_{1+}(\\cdot w) > 0\\right\\}\\|}{V N_{1+}(\\cdot \\cdot)}.\n",
    "$$\n",
    "\n",
    "Далее мы используем реккурентное соотношение\n",
    "\n",
    "$$\n",
    " \\hat P_{KN}(w_{N} \\mid w_1^{N-1}) = \\frac{{\\rm max}\\{c(w_1^N) - \\delta, 0\\}}{\\sum_{w_N}c(w_1^{N-1}w_N)} + \\frac{\\delta}{\\sum_{w_N}c(w_1^{N-1}w_N)}N_{1+}(w_1^{N-1}\\cdot)\\hat P_{KN}(w_N \\mid w_2^{N-1}).\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KneserNeyProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = ...\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): KneserNey parameter.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, storage, delta=0.5):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        self.__Ndots = len(self.__storage[2])\n",
    "        self.__V = len(self.__storage[1])\n",
    "        self.__rec_start, self.__smoothing = self.precalc()\n",
    "        self.__smoothing  =  self.__delta * self.__smoothing /self.__V / self.__Ndots\n",
    "        for word in self.__rec_start.keys():\n",
    "            self.__rec_start[word] =  (self.__rec_start[word] - self.__delta) / self.__Ndots\n",
    "            \n",
    "        self.__counter_ngram = self.count_begin()\n",
    "        self.__number_ngram = self.number_begin()\n",
    "        \n",
    "    \n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "    \n",
    "    def count_begin(self):\n",
    "        beginsCounter = Counter()\n",
    "        for gramSize in range(self.__storage.max_n):\n",
    "            for ngram in self.__storage[gramSize + 1]:\n",
    "                beginsCounter[ngram[:-1]] += self.__storage(ngram)\n",
    "        return beginsCounter\n",
    "    \n",
    "    def number_begin(self):\n",
    "        beginsNum = Counter()\n",
    "        for Size in range(self.__storage.max_n):\n",
    "            for ngram in self.__storage[Size + 1]:\n",
    "                beginsNum[ngram[:-1]] += 1\n",
    "        return beginsNum\n",
    "    \n",
    "    \n",
    "    \n",
    "    def precalc(self):\n",
    "        N_plus = Counter()\n",
    "        NN = []\n",
    "        for pair in self.__storage[2].keys():\n",
    "            NN.append(pair[1])\n",
    "            N_plus[str(pair[1])] += 1\n",
    "        return N_plus, len(np.unique(np.array(NN)))\n",
    "        \n",
    "    def KN(self, word, context):\n",
    "        if len(context) == 0:\n",
    "            prob = self.__rec_start[word] + self.__smoothing\n",
    "        else:\n",
    "            count = self.__counter_ngram[context]\n",
    "            if count == 0:\n",
    "                prob = self(word, context[1:])\n",
    "            else:\n",
    "                s_1 = max(self.__storage(context + (word, )) - self.__delta, 0)\n",
    "                s_2 = self.__delta * self.__number_ngram[context] * self.KN(word, context[1:])\n",
    "                prob = (s_1 + s_2) / count\n",
    "        return prob\n",
    "    \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a string!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        prob = self.KN(word, context)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KN estimator perplexity = 90.00797094386152\n",
      "3.901657316659065e-06\n",
      "1.727320227186788e-13\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "kn_estimator = KneserNeyProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('KN estimator perplexity = {}'.format(perplexity(kn_estimator, test_sents)))\n",
    "print(kn_estimator.prob('To be'.split()))\n",
    "print(kn_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение языка документа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Постановка задачи:**  \n",
    "Одна из задач, которая может быть решена при помощи языковых моделей $-$ **определение языка документа**. Реализуйте два классификатора для определения языка документа:\n",
    "1. Наивный классификатор, который будет учитывать частотности символов и выбирать язык текста по признаку: распределение частот символов \"наиболее похоже\" на распределение частот символов в выбранном языке.\n",
    "2. Классификатор на основе языковых моделей. Сами придумайте, как он должен работать.  \n",
    "_Подсказка_: лучше считать n-грамы не по словам, а по символам.\n",
    "\n",
    "---\n",
    "\n",
    "**Как представлены данные:**  \n",
    "Во всех текстовых файлах на каждой строчке записано отдельное предложение.\n",
    "1. В папочке _data_ находятся две папочки: _full_ и _plain_. В _full_ находятся тексты в той форме, что они были взяты из сети, в _plain_ находятся те же самые тексты, но с них сначала была снята диакритика, а затем русский и греческий тексты были транслитерованы в английский.\n",
    "2. В каждой из папочек _full_ и _plain_ находятся папочки _train_ и _test_.\n",
    "3. В _train_ находятся файлы с текстами с говорящими именами, например, _ru.txt_, _en.txt_.\n",
    "4. В _test_ находятся файлы _1.txt_, _2.txt_, $\\dots$ в которых хранятся тексты, язык которых нужно определить. В этой же папочке находится файл _ans.csv_, в котором вы можете найти правильные ответ и проверить, насколько хорошо сработали Ваши алгоритмы.\n",
    "\n",
    "---\n",
    "\n",
    "**Что нужно сделать:**  \n",
    "Напишите два своих классификатора (которые описаны в постановке задачи) и получите максимально возможное accuracy на test-сете. Разрешается использовать только _train_ для обучения.\n",
    "\n",
    "---\n",
    "\n",
    "**В данном задании мы не предоставляем стартового кода!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании я решила создать классификатор языка, который бы классифицировал текст, относительно частоты встречающихся в нем kgram. В частве метрики качества я выбрала логарифм правдаподобия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loglikelihood(model,kgrams):\n",
    "    probs = map(lambda kgram: model[kgram]+1e-4 , kgrams)\n",
    "    probs= [math.log(i) for i in probs]\n",
    "    return np.sum(probs)\n",
    "\n",
    "def normalize(counter):\n",
    "    norm= float(sum(counter.values()))\n",
    "    for key in counter:\n",
    "        counter[key]/=norm\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При инициализации  LanguageEstimator подается адрес папки, в которой хранятся текстовые документы для обучения. Для каждого из документов наш классификатор строит языковую модель, перечисляет частоты всех встречающихся kgram.\n",
    "Для предсказания языков классификатору на вход подается адрес папки, в которой содержатся документы, языки которых требуется предсказать, а также список ответов. Классификатор определяет языки всех документов, а затем считает accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageEstimator:\n",
    "    \"\"\"Class which predicts the language of the given text\n",
    "    according to the frequency of kgrams (letter kgrams)\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, dirname, k = 1):\n",
    "        language_model = {}\n",
    "    \n",
    "        self.__k = k\n",
    "        for fname in os.listdir(dirname):\n",
    "            faddr = os.path.join(dirname,fname)\n",
    "    \n",
    "            if not fname.endswith('.txt'):\n",
    "                continue\n",
    "    \n",
    "            with codecs.open(faddr,encoding='utf-8') as textfile:\n",
    "                text = textfile.read()\n",
    "    \n",
    "            #k = 1\n",
    "            kgrams = [text[i:i+k] for i in range(len(text)-k)]\n",
    "            model = normalize(Counter(kgrams))\n",
    "            lang_name = fname[:-4]\n",
    "            language_model[lang_name] = model\n",
    "        self.__language_model = language_model\n",
    "        print(self.__language_model.keys())\n",
    "    \n",
    "    def __call__(self, testdirname, ans):\n",
    "        k = self.__k\n",
    "        true_pred = 0\n",
    "        name_list = os.listdir(testdirname)\n",
    "        for fname in name_list:   \n",
    "            name = fname[:-4]\n",
    "            faddr = os.path.join(testdirname,fname)\n",
    "    \n",
    "            with codecs.open(faddr, encoding='utf-8') as textfile:\n",
    "                text = textfile.read()\n",
    "    \n",
    "            kgrams = [text[i:i+k] for i in range(len(text)-k)]\n",
    "\n",
    "            est_lang = 'pl'\n",
    "            est_log = loglikelihood(self.__language_model[est_lang],kgrams)\n",
    "            for pred_lang in self.__language_model.keys():\n",
    "                pred_log = loglikelihood(self.__language_model[pred_lang],kgrams)\n",
    "                if  pred_log > est_log:\n",
    "                    est_log = pred_log\n",
    "                    est_lang = pred_lang\n",
    "            print(fname, est_lang, ans_df[1][int(name)], est_log)\n",
    "            print(text[:100])\n",
    "            if (est_lang == ans_df[1][int(name)]):\n",
    "                true_pred += 1\n",
    "    \n",
    "    \n",
    "            print('\\n')\n",
    "        accuracy = 1.0 * true_pred/len(name_list)\n",
    "        return(accuracy)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для \"упрощенного набора\" из папки plain я создала языковую модель, классифицирующую тексты на основе частотности однограммов (то есть символов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans_df = pd.DataFrame.from_csv(\"C:\\\\Users\\\\Maria\\\\Desktop\\\\data\\\\ans.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ru', 'fr', 'el', 'sv', 'it', 'nl', 'eo', 'pl', 'no', 'es', 'ca', 'de', 'en', 'fi', 'pt', 'hu'])\n"
     ]
    }
   ],
   "source": [
    "dirname = 'C:\\\\Users\\\\Maria\\\\Desktop\\\\data\\\\plain\\\\train'\n",
    "language_model_uni = LanguageEstimator(dirname, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как она работает. При предсказании я печатала первые строки документа, его намер, предсказанный язык и язык, к которому действительно относится документ. Также выводила на печать логарифм максимального правдоподобия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt ca ca -11134.1486095\n",
      "-Alla on caura aquesta sageta, enterreu-hi el pobre Robin Hood sota el bosc que treu ufana.\n",
      "-Gaire… \n",
      "\n",
      "\n",
      "10.txt ca ca -57426.018903\n",
      "-Allargueu els ricos al guanet de l'arbre mestre!\n",
      "Un pasme intrigat es desvetlla en tota cara de l'a\n",
      "\n",
      "\n",
      "100.txt fi fi -51103.9817789\n",
      "Kuja johti suurehkolle ruohokentalle, ja talo oli edessamme.\n",
      "Times-lehtea lukevat harvoin muut kuin \n",
      "\n",
      "\n",
      "101.txt fi fi -41496.8953159\n",
      "Meilla on kumminkin useita johtolankoja kasissamme, ja on hyvin luultavaa, etta joku niista vie totu\n",
      "\n",
      "\n",
      "102.txt fi fi -475.344704392\n",
      "Tahtoisitko Bradleyn ohi kulkiessasi pyytaa hanta lahettamaan naulan vakevinta tupakkaansa tanne min\n",
      "\n",
      "\n",
      "103.txt fi fi -33173.2669178\n",
      "\"En joutanut edes panemaan hattua paahani.\n",
      "\"Kysymys on kai jostakin salametsastyksesta, luulisin\", s\n",
      "\n",
      "\n",
      "104.txt fi fi -5280.62632089\n",
      "Yhdessa kiiruhdimme alas portaita kadulle.\n",
      "Mina muistin tuon omituisen varotuksen, joka oli leikattu\n",
      "\n",
      "\n",
      "105.txt fi fi -58731.2404045\n",
      "Ja tuon liekin luona nyt tama roisto loikoo odottamassa.\n",
      "Times-lehtea lukevat harvoin muut kuin hyvi\n",
      "\n",
      "\n",
      "106.txt fr fr -2303.98954185\n",
      "C'est qu'en effet, ce plomb etait sorti d'une arme a feu, et quel autre qu'un etre humain avait pu s\n",
      "\n",
      "\n",
      "107.txt fr fr -21292.6901329\n",
      "Un pli amer apparut au coin des levres du visiteur.\n",
      "Milady comprit que Lord de Winter mentait et n'e\n",
      "\n",
      "\n",
      "108.txt fr fr -4320.76006567\n",
      "Ce procede acheva de desesperer Candide; il avait a la verite essuye des malheurs mille fois plus do\n",
      "\n",
      "\n",
      "109.txt fr fr -60398.1320457\n",
      "Si j'avais tue du gibier au-dela, de ma consommation, il m'aurait fallu l'abandonner au chien ou aux\n",
      "\n",
      "\n",
      "11.txt ca ca -26190.3653713\n",
      "-Es tan bonic, sempre, una forada!\n",
      "Continuava la quietud solemnial.\n",
      "Aixi es que interiorment resolgu\n",
      "\n",
      "\n",
      "110.txt fr fr -6500.22675698\n",
      "Ses roues rapides, bruyantes, battant l'eau quiretombait en ecume, lui donnaient un air de hate, un \n",
      "\n",
      "\n",
      "111.txt fr fr -34022.9412957\n",
      "Laurent, d'un temperament plus epais, tout en cedant a ses terreurs et a ses desirs, entendait raiso\n",
      "\n",
      "\n",
      "112.txt fr fr -82582.5588528\n",
      "—Si la soupe n'est pas prete, ca se comprend.\n",
      "Son menton etait appuye sur ses genoux, que ses deux b\n",
      "\n",
      "\n",
      "113.txt fr fr -28253.4619803\n",
      "– Il ne peut en etre autrement, procurateur, repondit Afranius sans rire, et meme d’un ton severe.\n",
      "L\n",
      "\n",
      "\n",
      "114.txt fr fr -71436.692815\n",
      "C’etait un vaste terrain inculte avec des buissons et des broussailles, une contree sterile, que Gle\n",
      "\n",
      "\n",
      "115.txt fr fr -34788.3048125\n",
      "Ou en etions-nous?\n",
      "--En ce moment elle evolue dans l'avant-port…\n",
      "He was in hopes of seeing Clelia ag\n",
      "\n",
      "\n",
      "116.txt fr fr -73125.5178154\n",
      "Tout d’abord, il ramassa le passeport et le tendit a Maximilien Andreievitch, qui le prit d’une main\n",
      "\n",
      "\n",
      "117.txt fr fr -82701.5682662\n",
      "Lorsque Alzire et les enfants furent la, elle partagea le vermicelle dans trois petites assiettes.\n",
      "–\n",
      "\n",
      "\n",
      "118.txt fr fr -40504.7224008\n",
      "-- Vous arrive-t-il souvent de vous promener avant le dejeuner, Mr Horscroft, demanda-t-elle, toujou\n",
      "\n",
      "\n",
      "119.txt fr fr -26146.2124584\n",
      "Non!\n",
      "Bon !...\n",
      "-- Etrange desir, madame Reed!\n",
      "Deja tout me parait moins facile.\n",
      "Elle lui conviendra t\n",
      "\n",
      "\n",
      "12.txt ca ca -72366.5059115\n",
      "No va aconseguir un somriure ni una resposta: aixi es que recaigue en el silenci i deixa que son cor\n",
      "\n",
      "\n",
      "120.txt fr fr -34862.6367522\n",
      "La mer etait calme alors, et j'avais grande envie de m'aventurer dans ma pirogue jusqu'au navire.\n",
      "Or\n",
      "\n",
      "\n",
      "121.txt hu hu -66798.4365278\n",
      "A jelen pillanatban is azt hajtogatta ugyan magaban, hogy a halal fia, de esze agaban sem volt szep \n",
      "\n",
      "\n",
      "122.txt hu hu -87208.0928176\n",
      "- Hat, ha igazan ezt tartja - mondta Magnus ur -, akkor elarulok onnek egy kis titkot: en magam is r\n",
      "\n",
      "\n",
      "123.txt hu hu -56504.9380738\n",
      "Szvijazsszkij asitozva kiserte ot ki az eloszobaba es csodalkozott azon a furcsa allapoton, a melybe\n",
      "\n",
      "\n",
      "124.txt hu hu -68852.6756933\n",
      "A csudovszkij kolostorbol valok?\n",
      "Egy darabig tetovazott, aztan mas iranyba fordult s elindult vaktab\n",
      "\n",
      "\n",
      "125.txt hu hu -87205.7101034\n",
      "Egyszeruen az uregen oly hosszu ideig atrobajlo aradat utolso nyomai voltak; a levego pedig enyhe ny\n",
      "\n",
      "\n",
      "126.txt hu hu -46088.3914317\n",
      "Ne vedd rossz neven.\n",
      "Ezek allatborokbe varrt keresztenyek voltak.\n",
      "Fegyverek dontik el, vajon civiliz\n",
      "\n",
      "\n",
      "127.txt hu hu -84925.1726094\n",
      "- Hamar, barataim! - kialtotta. - Elfaradt!\n",
      "Tom ebren fekudt, es nyugtalan turelmetlenseggel vart.\n",
      "E\n",
      "\n",
      "\n",
      "128.txt hu hu -87767.2772167\n",
      "Ez a muszer mozgathato tukrok segitsegevel meghatarozza a nap es a latohatar altal bezart szoget, po\n",
      "\n",
      "\n",
      "129.txt hu hu -95277.5437449\n",
      "- Csak lassan, lassan; igazan nincs mert olyan nagyon sietnie oda, ahonnet a legtobb ember kifele ki\n",
      "\n",
      "\n",
      "13.txt ca ca -10121.0329273\n",
      "Ho esborra, i s'enfurisma amb si mateix per la seva feblesa.\n",
      "Es una cosa de no-res.\n",
      "-Darrera els oms\n",
      "\n",
      "\n",
      "130.txt hu hu -57411.2343455\n",
      "Bizony, megoregedett, - jegyezte meg a kamaras.\n",
      "Hiszek!...: Krisztusom!\n",
      "Panaszkodott, hogy rosszul m\n",
      "\n",
      "\n",
      "131.txt hu hu -50272.6930895\n",
      "Fejem folott gomolygo felhok szalltak, optikai csalodast okozva: ugy ereztem, mintha mozdulatlanok v\n",
      "\n",
      "\n",
      "132.txt hu hu -1499.87030581\n",
      "Vad lo is megjuhaszodik,\n",
      "Haladok, lordom!\n",
      "Ha igy van, annyi aranyat adok Bussant Raonak, amennyi a s\n",
      "\n",
      "\n",
      "133.txt hu hu -66120.9869282\n",
      "Ez mar dofi! - dormogte Pencroff.\n",
      "Szepen elrendezett, nagy kert volt ez, s Collins maga muvelte.\n",
      "\"Le\n",
      "\n",
      "\n",
      "134.txt hu hu -37466.4120375\n",
      "Valamennyi utas osszegyult a tarsalgoban.\n",
      "- Nincs megirva, hogy mindenki ugy vegezze az eletet... mi\n",
      "\n",
      "\n",
      "135.txt hu hu -15257.0755901\n",
      "Hacsak foglalatban nem akarta valaki latni oket, ennel tobbet valoban nem kivanhatott.\n",
      "A fiatal Scse\n",
      "\n",
      "\n",
      "136.txt it it -61583.8863601\n",
      "Questo preparativo di legatura e la vergogna che per me ne derivava, calmarono la mia agitazione.\n",
      "Tu\n",
      "\n",
      "\n",
      "137.txt it it -53745.9673543\n",
      "Bevvero un bicchierino per uno e tornarono alla tavola.\n",
      "Levin, accigliato, stava seduto, ascoltando \n",
      "\n",
      "\n",
      "138.txt it it -88708.2344529\n",
      "Sara proprio una cosa straordinaria!\n",
      "Io voglio a questo proposito addurre uno esemplo moderno.\n",
      "Ella \n",
      "\n",
      "\n",
      "139.txt it it -25024.7659012\n",
      "— Dopo una giovinezza e una virilita trascorse ora in mezzo a inesprimibili sofferenze, ora in una d\n",
      "\n",
      "\n",
      "14.txt ca ca -63114.1642095\n",
      "-Sst!\n",
      "Els minyons de dalt estaven tan excitats i tan delectats com ells mateixos.\n",
      "No ho voldria pas,\n",
      "\n",
      "\n",
      "140.txt it it -36384.2244984\n",
      "I passi veloci scricchiolanti del servitore che camminava nel salotto lo fecero tornare in se.\n",
      "Il fa\n",
      "\n",
      "\n",
      "141.txt it it -5241.70793752\n",
      "Ma quelli che parlavano tacquero, e la sua domanda sconveniente fu udita.\n",
      "Trovai allora il vascello \n",
      "\n",
      "\n",
      "142.txt it it -16148.9064289\n",
      "Fui trasportata in sogno nella mia fanciullezza; mi pareva di trovarmi nella camera rossa di Gateshe\n",
      "\n",
      "\n",
      "143.txt it it -671.624911868\n",
      "Ma nel piccolo ingresso si fermo per riflettere a quello che era accaduto.\n",
      "A mezzanotte torno gelato\n",
      "\n",
      "\n",
      "144.txt it it -66081.5396929\n",
      "Dopo cio tornai alla mia fortezza, ove mi posi a lavorare per lui; e prima di tutto gli diedi un pai\n",
      "\n",
      "\n",
      "145.txt it it -87922.87475\n",
      "Ma cosa mai!\n",
      "— E unita anche cosi — ella rispose in modo appena percettibile.\n",
      "L’ha detto del resto: \n",
      "\n",
      "\n",
      "146.txt it it -14021.8029344\n",
      "I duri mattoni rossi non si son che consolidati col tempo, e le loro scale di quercia non scricchiol\n",
      "\n",
      "\n",
      "147.txt it it -34954.0251265\n",
      "Ella lo guardo diritto nel viso, quasi supplicandolo di farle grazia, e gli porse la mano.\n",
      "Vidi dise\n",
      "\n",
      "\n",
      "148.txt it it -96771.4192845\n",
      "Liberata da quell'oppressione, stabilii di lavorare con nuova lena e di aprirmi la via a traverso og\n",
      "\n",
      "\n",
      "149.txt it it -89426.7736603\n",
      "Io che mi vidi ridotto al mero stato di natura, io la capii con mio giornaliero scoraggiamento quest\n",
      "\n",
      "\n",
      "15.txt ca ca -27673.3948675\n",
      "Aixi es com ho fan els nens i les nenes bons minyons.\n",
      "Hi hague silenci per algun temps.\n",
      "-Molt be, Hu\n",
      "\n",
      "\n",
      "150.txt it it -92271.2487109\n",
      "Tre cose m’incoraggiavano: primieramente un dolce placido mare, in secondo luogo la marea che saliva\n",
      "\n",
      "\n",
      "151.txt nl nl -13983.5688244\n",
      "Het was het onbrandbare hout, dat Paganel had opgenoemd onder de zonderlinge voortbrengselen van Aus\n",
      "\n",
      "\n",
      "152.txt nl nl -67983.6833415\n",
      "Wat is dat?\"\n",
      "Gedurende dien tijd vroeg Ned Land, die niets met schelpen ophad, mij naar mijn onderho\n",
      "\n",
      "\n",
      "153.txt nl nl -31998.2349572\n",
      "Ik stond spoedig op, kleedde mij en ging naar het salon.\n",
      "De wilden dooden hun vijanden en eten ze op\n",
      "\n",
      "\n",
      "154.txt nl nl -29596.0155089\n",
      "--„En van wien hadt gij dien saffier, Athos?”\n",
      "Welligt kon hij zich nog niet voorstellen, dat het gev\n",
      "\n",
      "\n",
      "155.txt nl nl -19664.6889638\n",
      "\"Je moogt toch stellig om vergeving vragen,\" zeide Elinor, \"omdat je haar verdriet hebt gedaan; en i\n",
      "\n",
      "\n",
      "156.txt nl nl -102003.376724\n",
      "Waren wij bij Spitsbergen of bij Nova Zembla?\n",
      "Gij zult de kist dus moeten openbreken.\n",
      "Op dien stille\n",
      "\n",
      "\n",
      "157.txt nl nl -96364.6841943\n",
      "Ik hoorde den armen jongen hijgen; zijn ademhaling werd kort en gejaagd.\n",
      "Ditmaal echter werd zijn aa\n",
      "\n",
      "\n",
      "158.txt nl nl -5530.84922358\n",
      "Maar de Nautilus had niets met die leelijke dieren uit te staan.\n",
      "Al haar krachten verzamelend, hoewe\n",
      "\n",
      "\n",
      "159.txt nl nl -25782.1452361\n",
      "Ik kon den slaap nog zoo gemakkelijk niet vatten; te veel denkbeelden doorkruisten mijn geest, te ve\n",
      "\n",
      "\n",
      "16.txt de de -104285.780263\n",
      "Leah schuttelte den Kopf, und hier nahm die Unterhaltung ein Ende.\n",
      "»Ein Herr ist hier, Nikolai Dmitr\n",
      "\n",
      "\n",
      "160.txt nl nl -26494.7692919\n",
      "Dit mocht wel heeten de stijfhoofdigheid tot het uiterste te drijven.\n",
      "Och heere, geen wonder, dat ze\n",
      "\n",
      "\n",
      "161.txt nl nl -91773.9639487\n",
      "Allen zagen hem met vragende blikken aan.\n",
      "Het was hem alsof de middag-vacantie nooit zou komen. 't W\n",
      "\n",
      "\n",
      "162.txt nl nl -7051.43393546\n",
      "\"Wij gaan vertrekken,\" zeide hij.\n",
      "Zouden wij nu op eene plaats komen, waar die verschijnselen zich i\n",
      "\n",
      "\n",
      "163.txt nl nl -81365.3122574\n",
      "Was het door overbrenging van krachten in een tot nog toe onbekend stelsel van hefboomen, dat men di\n",
      "\n",
      "\n",
      "164.txt nl nl -8180.66082452\n",
      "„O, indien zulks waar mocht zijn, ik zou er krankzinnig door worden.”\n",
      "Deze overdenkingen deelde ik a\n",
      "\n",
      "\n",
      "165.txt nl nl -39579.9346497\n",
      "„Indien ik haar eens mijn dienst ging aanbieden!” dacht hij.\n",
      "Bedenk u wel, Wilson! de minuten hebben\n",
      "\n",
      "\n",
      "166.txt no no -41351.5872324\n",
      "Jeg ventet til han var kommet ut av syne, og fulgte sa etter ham.\n",
      "Det var et listig pafunn, for bort\n",
      "\n",
      "\n",
      "167.txt no no -4979.16261979\n",
      "Men jeg kunne ikke finne noe bedre a gjøre enn a iaktta ham fra høyden her og lette min samvittighet\n",
      "\n",
      "\n",
      "168.txt no no -50496.513649\n",
      "Jeg forsikrer Dem, De ville ikke ha den minste utsikt til a komme levende tilbake.\n",
      "Jeg ønsker bare a\n",
      "\n",
      "\n",
      "169.txt no no -38004.3167222\n",
      "Denne gangen tok han meg ganske nydelig ved nesen!\n",
      "Bundet til denne søylen var en skikkelse som var \n",
      "\n",
      "\n",
      "17.txt de de -64378.9102235\n",
      "So verdarb ihn Emma noch aus ihrem Grabe heraus.\n",
      "Denn alle Reisenden stimmen in dem Punkte miteinand\n",
      "\n",
      "\n",
      "170.txt no no -49782.0033879\n",
      "Den dystre, lange gangen fører gjennom klippede hekker som star som høye murer pa begge sidene, med \n",
      "\n",
      "\n",
      "171.txt no no -10238.8831401\n",
      "Vi har endvidere to forpaktere.\n",
      "“Mannen er jo en fare for samfunnet.\n",
      "“Hva det siste angar kan jeg ik\n",
      "\n",
      "\n",
      "172.txt no no -5540.82624925\n",
      "Fikk noen andre ogsa noe?”\n",
      "De tror da vel ikke pa slikt tapelig snakk?” sa jeg.\n",
      "“Nei, nar De endelig\n",
      "\n",
      "\n",
      "173.txt no no -60234.5102133\n",
      "“Der var ingen som kunne vite at vi skulle ta inn pa dette hotellet.”\n",
      "Veien i midten er omkring fire\n",
      "\n",
      "\n",
      "174.txt no no -59928.3532935\n",
      "Vart hell og hans liv kan avhenge av at han kommer ut før taken nar stien.”\n",
      "“Helt oppe i fjerde losj\n",
      "\n",
      "\n",
      "175.txt no no -19526.7291062\n",
      "De burde virkelig skamme Dem.\n",
      "I samme øyeblikk dro sir Henry frøken Stapleton hen til seg.\n",
      "“Skal De \n",
      "\n",
      "\n",
      "176.txt no no -49689.3541593\n",
      "“Djevelens hjelpere ma vel være vesener av kjøtt og blod, ikke sant?\n",
      "“De har latt et par tommer av d\n",
      "\n",
      "\n",
      "177.txt no no -55929.8083894\n",
      "Han kunne gjøre krav pa eiendommen fra Syd-Amerika, godtgjøre sin identitet overfor de derværende br\n",
      "\n",
      "\n",
      "178.txt no no -61928.7606776\n",
      "“Om De hadde fatt vite noe, kunne det kun ha hjulpet oss til at jeg ble røpet.\n",
      "Den ene ble altsa stj\n",
      "\n",
      "\n",
      "179.txt no no -66287.8341082\n",
      "Men skulle det verste være hendt, skal vi hevne ham!”\n",
      "“Det er et halvt ark alminnelig papir uten sa \n",
      "\n",
      "\n",
      "18.txt de de -47058.087484\n",
      "– Ganz gewiß, unterbrach ihn Glenarvan.\n",
      "»Ach, was ich durchgemacht habe!\n",
      "Anna Karenina\n",
      "Lieber in den\n",
      "\n",
      "\n",
      "180.txt no no -56433.4479001\n",
      "Jeg trodde at De var i Baker Street og var fullt opptatt av injuriesaken.”\n",
      "Det var apenbart at han h\n",
      "\n",
      "\n",
      "181.txt pl pl -46554.5230067\n",
      "-- Wszyscy obywatele Devonshire dziela panskie zdanie -- wtraciłem. -- Zadna okolica nie wzbudza tak\n",
      "\n",
      "\n",
      "182.txt pl pl -6205.31228374\n",
      "Do Merripit-House pojedziesz pan amerykanem.\n",
      "-- Szatanski pomysł! -- mowił Holmes, nachylajac sie na\n",
      "\n",
      "\n",
      "183.txt pl pl -64704.2541493\n",
      "Nasi klienci stawili sie punktualnie.\n",
      "Był daleko blizej od młodej pary, niz ja i widocznie podazał k\n",
      "\n",
      "\n",
      "184.txt pl pl -8837.86982846\n",
      "Odetchneła swobodniej. -- Dzieki Bogu!\n",
      "Pociagniemy go do odpowiedzialnosci!\n",
      "Musze pana objasnic, ze \n",
      "\n",
      "\n",
      "185.txt pl pl -47908.1464039\n",
      "Ale manuskrypt jest krotki i ma scisły zwiazek z ta sprawa.\n",
      "„Taka jest, moi synowie, legenda o psie,\n",
      "\n",
      "\n",
      "186.txt pl pl -23705.0649897\n",
      "To nie zapowiada nic dobrego dla Baskervillow.\n",
      "-- To był zachwyt znawcy... -- rzekł po chwili, wskaz\n",
      "\n",
      "\n",
      "187.txt pl pl -3210.98195306\n",
      "Przeskakiwał z kamienia na kamien, ze skały na skałe, jak dziki kozioł.\n",
      "Ale musi oswoic sie z ta mys\n",
      "\n",
      "\n",
      "188.txt pl pl -9387.57538101\n",
      "Poniewaz sie spozniał, wyszedłem na jego spotkanie i wtedy usłyszałem okrzyk...\n",
      "W chwili, gdys mnie \n",
      "\n",
      "\n",
      "189.txt pl pl -42101.9572518\n",
      "Reszta kapitałow przeszła na sir Henryka. -- Ile wynosza?\n",
      "To nie zapowiada nic dobrego dla Baskervil\n",
      "\n",
      "\n",
      "19.txt de de -99590.8171605\n",
      "Die Jungen horchten und warteten.\n",
      "»Ach ja!\n",
      "»Nur mich.«\n",
      "Er fand sie schon, reizend!\n",
      "Tom war bis dahin\n",
      "\n",
      "\n",
      "190.txt pl pl -41489.2124347\n",
      "Gdziez jest ten pies przeklety?\n",
      "-- Przyznaje, ze to jest prawdopodobniejsze.\n",
      "-- Brode? -- podchwycił\n",
      "\n",
      "\n",
      "191.txt pl pl -7774.071896\n",
      "„Słyszac to Hugon, wybiegł przed dom, zwołał chłopcow stajennych, kazał im osiodłac swoja klacz ulub\n",
      "\n",
      "\n",
      "192.txt pl pl -8197.67046887\n",
      "-- Skrecił kark, spadajac z tej skały.\n",
      "-- Tak.\n",
      "-- A teraz, panie Holmes -- rzekł -- pokaze panu cos \n",
      "\n",
      "\n",
      "193.txt pl pl -13425.4872365\n",
      "Zeby sa za duze na jamnika, za małe na wyzła.\n",
      "-- Nie mamy zadnych wiesci -- odparł doktor Mortimer w\n",
      "\n",
      "\n",
      "194.txt pl pl -45009.6964605\n",
      "Biedna kobieta ma ciezkie zycie...\n",
      "Nie wiedziałem, ze pani Lyons chce sie rozwodzic.\n",
      "-- Nie.\n",
      "Oswiadc\n",
      "\n",
      "\n",
      "195.txt pl pl -44994.0310374\n",
      "-- Coz to za jeden?\n",
      "Co do mojej, moge reczyc, ze nie płakała.\n",
      "Nad sienia była oszklona galerya.\n",
      "Sa p\n",
      "\n",
      "\n",
      "196.txt pt pt -55246.3662323\n",
      "\"A Rainha de Copas, ela fez algumas tortas, Todas em um um dia de verao: o Valete de Copas, ele roub\n",
      "\n",
      "\n",
      "197.txt pt pt -43959.241911\n",
      "\"Quem liga para voce?\" disse Alice, (que havia crescido para seu tamanho normal neste momento.)\n",
      "Alic\n",
      "\n",
      "\n",
      "198.txt pt pt -82856.0840529\n",
      "Alguns dos passaros sairam correndo de uma vez: uma velha pega-rabuda comecou a se embrulhar toda mu\n",
      "\n",
      "\n",
      "199.txt pt pt -18815.9226214\n",
      "'Nao completamente certo, receio', disse Alice, timidamente; 'algumas das palavras foram alteradas.'\n",
      "\n",
      "\n",
      "2.txt ca ca -37196.9707684\n",
      "Eren satisfactoris, i hi hague barata de propietat.\n",
      "-No, Huck: no hi corre: rodara per l'indret on v\n",
      "\n",
      "\n",
      "20.txt de de -15604.933508\n",
      "Sie fuhlte sich ganz vernichtet, und das malte sich auch auf ihrem Gesichte.\n",
      "»Ihre Gefuhle konzentri\n",
      "\n",
      "\n",
      "200.txt pt pt -24950.2384614\n",
      "'Traga-me minhas luvas neste instante!'\n",
      "\"Voce pode ir\", disse o Rei, e o Chapeleiro rapidamente deix\n",
      "\n",
      "\n",
      "201.txt pt pt -11715.0707361\n",
      "\"Voce nao tem o direito de crescer aqui\", disse o Arganaz.\n",
      "Os olhos do chapeleiro abriram-se amplame\n",
      "\n",
      "\n",
      "202.txt pt pt -67683.4505967\n",
      "Entao eles comecaram a dancar solenemente em volta da Alice, sempre pisando nos pes dela quando pass\n",
      "\n",
      "\n",
      "203.txt pt pt -73137.0931771\n",
      "O Lacaio pareceu achar ser esta uma boa oportunidade para repetir sua observacao, com variacoes.\n",
      "\"Em\n",
      "\n",
      "\n",
      "204.txt pt pt -33027.0767332\n",
      "\"Evidente que nao!\" disse o Chapeleiro com ar de desdem.\n",
      "\"O que e que elas desenhavam?\" perguntou Al\n",
      "\n",
      "\n",
      "205.txt pt pt -264.922296569\n",
      "\"Porque\", disse o Grifo, \"voce primeiro forma uma linha ao longo do fundo do mar...\"\n",
      "\n",
      "\n",
      "\n",
      "206.txt pt pt -69164.6657721\n",
      "\"Voce vai jogar croquete com a Rainha hoje?\"\n",
      "As rosas que dela nasciam eram brancas, mas tres jardin\n",
      "\n",
      "\n",
      "207.txt pt pt -785.214729642\n",
      "\"Agora nos devemos progredir melhor\".\n",
      "Alice pensou que isto era muito curioso, e aproximou-se para o\n",
      "\n",
      "\n",
      "208.txt pt pt -76300.250723\n",
      "'Eu nao tenho a menor ideia do que voce esta falando,' disse Alice.\n",
      "E a moral disso e - 'Passaros do\n",
      "\n",
      "\n",
      "209.txt pt pt -22212.1019645\n",
      "Entretanto, ela se levantou, e comecou a repetir, mas sua cabeca estava tao cheia da Quadrilha da La\n",
      "\n",
      "\n",
      "21.txt de de -18739.4686219\n",
      "Ling -- a, ling, ling!\n",
      "Ich will ihn sogleich holen.«\n",
      "»Was ist Ihnen?\n",
      "»Du siehst mich an«, sagte sie,\n",
      "\n",
      "\n",
      "210.txt pt pt -47479.4329142\n",
      "\"O Grifo levantou as duas patas em surpresa.\n",
      "Conte a ela sobre a zaao e tudo isto\", ele disse para o\n",
      "\n",
      "\n",
      "211.txt ru ru -75599.6345587\n",
      "– Ja v voshishhenii, – monotonno pel Korov'ev, – my v voshishhenii, koroleva v voshishhenii.\n",
      "Dazhe s\n",
      "\n",
      "\n",
      "212.txt ru ru -85020.8833411\n",
      "Poidet', poidet' i pridet'.\n",
      "Jeto sdelalos' samo soboi, -- skazala ona razdrazhitel'no, -- i vot... -\n",
      "\n",
      "\n",
      "213.txt ru ru -3502.13935608\n",
      "SMERT''\n",
      "Oni verjat v glaz, i v porchu, i v privoroty, a my...\n",
      "Tak vot, v jetom polozhenii drugaja zh\n",
      "\n",
      "\n",
      "214.txt ru ru -58795.5211827\n",
      "I ty zdes'? – i tut on polez zdorovat'sja.\n",
      "– Sjad', – molvil Pilat i ukazal na kreslo.\n",
      "Ja vsegda zav\n",
      "\n",
      "\n",
      "215.txt ru ru -16274.7528288\n",
      "– Rekomenduju vam... – nachal bylo Voland i sam sebja perebil: – Net, ja videt' ne mogu jetogo shuta\n",
      "\n",
      "\n",
      "216.txt ru ru -71663.8148926\n",
      "Chto za ohota sporit'?\n",
      "Chto tebe, chto vam nuzhno?\n",
      "-- Da, ja teper' vse ponjala, -- prodolzhala Dar'\n",
      "\n",
      "\n",
      "217.txt ru ru -22334.0976349\n",
      "Brosil vse -- kar'eru, menja, i tut-to ona eshhe ne pozhalela ego, a narochno ubila ego sovsem.\n",
      "-- C\n",
      "\n",
      "\n",
      "218.txt ru ru -53140.9586001\n",
      "On vse-taki vzjal risunok, polozhil k sebe na stol i, otdalivshis' i prishhurivshis', stal smotret' \n",
      "\n",
      "\n",
      "219.txt ru ru -79791.8749458\n",
      "U nas ustroilos'.\n",
      "Ja ne byla by toju zhe, da, no prostila by, i tak prostila by, kak budto jetogo ne\n",
      "\n",
      "\n",
      "22.txt de de -3982.03666749\n",
      "Aber die alte Frau predigte immer weiter und prophezeite, sie wurden alle beide im Armenhause enden.\n",
      "\n",
      "\n",
      "220.txt ru ru -42783.3236908\n",
      "-- O chem u vas nynche rech'? -- sprashival Levin, ne perestavaja ulybat'sja.\n",
      "Mne videt' merzko, mer\n",
      "\n",
      "\n",
      "221.txt ru ru -70093.7129695\n",
      "Chto takoe vinovat?\n",
      "– Pravda?\n",
      "Vse, chto ugodno, no tol'ko ne nevnimanie.\n",
      "-- Chto zh ty, vse hotel na\n",
      "\n",
      "\n",
      "222.txt ru ru -77171.5716982\n",
      "A.\n",
      "Podhodja k konjushne, on vstretilsja s belonogim ryzhim Gladiatorom Mahotina, kotorogo v oranzhev\n",
      "\n",
      "\n",
      "223.txt ru ru -46762.5284295\n",
      "Na ego zov v perednjuju vybezhal malen'kii, prihramyvajushhii, obtjanutyi chernym triko, s nozhom, z\n",
      "\n",
      "\n",
      "224.txt ru ru -78305.3894929\n",
      "Ja uletaju navsegda i prishel k vam lish' s tem, chtoby poproshhat'sja.\n",
      "Pervoe vremja posle togo, ka\n",
      "\n",
      "\n",
      "225.txt ru ru -79913.2420616\n",
      "-- Bez tebja bog znaet chto by bylo!\n",
      "V nei est' chto-to strannoe, -- govorila ee prijatel'nica.\n",
      "– Po\n",
      "\n",
      "\n",
      "226.txt sv sv -83314.4582941\n",
      "”Dar ser du!\n",
      "”Det ar en karl i min sang”, sade Georges far; ”jag har hans fotter pa min kudde.”\n",
      "I al\n",
      "\n",
      "\n",
      "227.txt sv sv -76669.1530825\n",
      "Vi var kvar under bron, pa exakt samma stalle dar vi varit da jag borjade ro och dar stod de dar bad\n",
      "\n",
      "\n",
      "228.txt sv sv -82146.6622159\n",
      "Sa slappte han taget med ett skrik, blek i ansiktet.\n",
      "1 pint kraftigt ol var sjatte timme.\n",
      "Harris”; o\n",
      "\n",
      "\n",
      "229.txt sv sv -55500.4037757\n",
      "Jag glomde fullkomligt bort mig.\n",
      "George, barande filtar och rockar, rokande en kort pipa.\n",
      "Vi kunde i\n",
      "\n",
      "\n",
      "23.txt de de -80230.9203345\n",
      "Und wenn sie nach einem Gesprache mit ihm sich bewußt wurden, daß eigentlich nichts besonders Vergnu\n",
      "\n",
      "\n",
      "230.txt sv sv -52390.5362711\n",
      "Annat i frukostvag som George foreslog var agg och bacon, vilket var enkelt att tillaga, kallskuret,\n",
      "\n",
      "\n",
      "231.txt sv sv -1355.83679722\n",
      "”Var ar hammaren?\n",
      "Vid andra tillfallen snurrade hans punt helt enkelt runt och stannade pa samma sta\n",
      "\n",
      "\n",
      "232.txt sv sv -34140.8378029\n",
      "Det var slutet for den festen.\n",
      "De brukade forbrylla mig en smula i borjan, men sa Herren min Skapare\n",
      "\n",
      "\n",
      "233.txt sv sv -71706.1054501\n",
      "”Vad vill du?” sager hon; ”ar det nagot som ar fel?”\n",
      "Nagonting har gatt snett; rodret har lossnat, b\n",
      "\n",
      "\n",
      "234.txt sv sv -13698.7584856\n",
      "Om ni fragar mig, sa skulle jag vilja kalla det hela for rena tokskapsdillerier-na.”\n",
      "Jag sade, att f\n",
      "\n",
      "\n",
      "235.txt sv sv -22554.0405566\n",
      "Jag glomde fullkomligt bort mig.\n",
      "George imponerade pa oss genom att ta med ombyte av underklader och\n",
      "\n",
      "\n",
      "236.txt sv sv -40026.9723929\n",
      "Sa, plotsligt, verkade den borja sla.\n",
      "Ha!\n",
      "Efter etthundra yards ar de naturligtvis andfadda och stan\n",
      "\n",
      "\n",
      "237.txt sv sv -84697.2022084\n",
      "Hor Du inte?\n",
      "Sa maste det ha gatt till, nar den karlekskranke gossen Henrik VIII uppvaktade sin lill\n",
      "\n",
      "\n",
      "238.txt sv sv -87848.286284\n",
      "Sa, den foljande aftonen var vi ater forsamlade, for att diskutera och utveckla vara planer.\n",
      "Till at\n",
      "\n",
      "\n",
      "239.txt sv sv -12971.0986009\n",
      "Vi tande vara pipor och satt, skadande ut i den tysta natten och samtalade.\n",
      "De kommer att kalla oss \n",
      "\n",
      "\n",
      "24.txt de de -92605.6853805\n",
      "Ein Brechreiz uberkam sie jetzt so plotzlich, daß sie kaum noch Zeit hatte, ihr Taschentuch unter de\n",
      "\n",
      "\n",
      "240.txt sv sv -19407.5218116\n",
      "Da tande vi lyktan och satte oss hukande till bords.\n",
      "”Ja, och hur manga slag hade ni vantat er?” sva\n",
      "\n",
      "\n",
      "25.txt de de -81765.4766211\n",
      "Von Republiken will ich nicht reden, weil dies von mir bereits in einem andern Werke ausfuhrlich ges\n",
      "\n",
      "\n",
      "26.txt de de -97119.0299616\n",
      "»Sehr.\n",
      "Sie hat sie hierher gesandt, um geheilt zu werden, wie die Juden des Altertums ihre Aussatzig\n",
      "\n",
      "\n",
      "27.txt de de -47578.0427806\n",
      "Der Prozeß war nichts anderes als ein großes Geschaft, wie er es schon oft mit Vorteil fur die Bank \n",
      "\n",
      "\n",
      "28.txt de de -52277.8529433\n",
      "Auch beschaftigte er sich damit, das schone Werk von Baudry-le-Rouge, des Bischofs von Noyon und Tou\n",
      "\n",
      "\n",
      "29.txt de de -14151.8821428\n",
      "»Nein, wirklich, es tut mir manchmal leid, daß ich Mamas Rat gefolgt bin.\n",
      "»Ei!« sagte Oudarde, »Schw\n",
      "\n",
      "\n",
      "3.txt ca ca -20674.5337892\n",
      "El meu desig fora que tinguessim una mena de descripcio de aquests bretols; aixo seria un gran ajut,\n",
      "\n",
      "\n",
      "30.txt de de -56363.0601618\n",
      "Du weißt, ich war stolz auf meine Kraft.\n",
      "In Romagna allein festgegrundete Herrschaft; mit allen ubri\n",
      "\n",
      "\n",
      "31.txt el el -83601.8511918\n",
      "Den pisteyw nachh ginh ws twra frichtoterh adikia.\n",
      "Tipote den einai dikaiotero apo to na metacheiriz\n",
      "\n",
      "\n",
      "32.txt el el -11123.4359545\n",
      "M' epiase tremoyla, san akoysa aythn thn apofash k' emasa tis liges dynameis, poy m ' apomeinan, gia\n",
      "\n",
      "\n",
      "33.txt el el -61142.7130512\n",
      "Fwnaxane amesws alloys dyo Ebraioys· o Agathoylhs, poylhse ki' alla diamantia· kai fygane oloi toys \n",
      "\n",
      "\n",
      "34.txt el el -81927.6204909\n",
      "— Ma thn pisth moy, eipe o adelfos Garoyfalhs tha pethymoysa ol' oi theatinoi na pnigoyne sto bathos\n",
      "\n",
      "\n",
      "35.txt el el -2255.93424876\n",
      "O anabaptisths bohthoyse ligaki sthn kybernhsi toy karabioy· htane panw sth gefyra: enas nayths thym\n",
      "\n",
      "\n",
      "36.txt el el -42500.6403778\n",
      "Fwnaxane amesws alloys dyo Ebraioys· o Agathoylhs, poylhse ki' alla diamantia· kai fygane oloi toys \n",
      "\n",
      "\n",
      "37.txt el el -54816.3423244\n",
      "Kai stis politeies poy fainontai pws chairontai thn eirhnh ki' opoy oi technes anthoyne, oi anthrwpo\n",
      "\n",
      "\n",
      "38.txt el el -73703.7174906\n",
      "— Eimai ekaton ebdomhnta dyo chronwn ki' ematha apo to makarith patera moy, ypaspisth toy basilia, t\n",
      "\n",
      "\n",
      "39.txt el el -14494.5515365\n",
      "Legetai, pws yparchoyn anthrwpoi poly eygenikoi s' ayth thn polh· as to pistepsoyme. — Oso gia mena,\n",
      "\n",
      "\n",
      "4.txt ca ca -43378.1956495\n",
      "Al cap de poc eixiren a l'escamot de mates de roldo, miraren estrategicament enfora, veieren la ribe\n",
      "\n",
      "\n",
      "40.txt el el -3600.04425497\n",
      "H gynaika toy rhtora eiche bgalei to kefali ths sto parathyro kai blepontas enan anthrwpo, poy amfeb\n",
      "\n",
      "\n",
      "41.txt el el -73703.2656527\n",
      "O perigoyrdinos abbas proseferthhke na ton eisagagh sthn hthopoio.\n",
      "Ach!\n",
      "Enas anthrwpos, poy den eich\n",
      "\n",
      "\n",
      "42.txt el el -7965.29578194\n",
      "KEFALAIO XXVII.\n",
      "Enw syzhtoysan o oyranos skoteiniase, oi anemoi fyshxan apo ta tessera shmeia toyran\n",
      "\n",
      "\n",
      "43.txt el el -79180.3274639\n",
      "— w! nai, apanthse, agapw tryfera th despoinida Kynegondh.\n",
      "— E! kala, toy eipe, agapate panta trela \n",
      "\n",
      "\n",
      "44.txt el el -48678.2188054\n",
      "Nomizete, pws prokeitai na perasete sth soybla ena ihsoyith ki' omws einai o yperaspisths sas, o ech\n",
      "\n",
      "\n",
      "45.txt el el -39195.4196311\n",
      "O pagglwsshs kata th therapeia echase mono tona mati kai ton' ayti.\n",
      "Ths arnhthhkane, opws len edw, t\n",
      "\n",
      "\n",
      "46.txt en en -634.775397066\n",
      "One day, after having inquired three times whether there were any letters, Madame de Fervaques sudde\n",
      "\n",
      "\n",
      "47.txt en en -16816.7673976\n",
      "When these things were a little over with her, we fell into a close debate about what should be firs\n",
      "\n",
      "\n",
      "48.txt en en -10440.4211881\n",
      "But if this were true, why had she preferred wandering and misery to his love, his tenderness, and a\n",
      "\n",
      "\n",
      "49.txt en en -47878.3014722\n",
      "He derived this repugnance from Rousseau's _Confessions_.\n",
      "\"What ain't a dream?\"\n",
      "That is settled.\"\n",
      "Ti\n",
      "\n",
      "\n",
      "5.txt ca ca -10960.9626363\n",
      "Parla qui-sap-lo, pero el parlar no fa mal: almenys no en fa si no es posa a gemegar.\n",
      "-Coratge, Tom!\n",
      "\n",
      "\n",
      "50.txt en en -70857.1056472\n",
      "Why, Roddy, what's amiss now?\"\n",
      "He read the letters, one of which impressed him unpleasantly.\n",
      "\"Lord!\"\n",
      "\n",
      "\n",
      "51.txt en en -18934.3018271\n",
      "I hobble not nor do I limp,\n",
      "'The Revolutionaries of Piedmont, of Spain, ought they to have compromis\n",
      "\n",
      "\n",
      "52.txt en en -50815.3232583\n",
      "\"It was easy to follow him,\" he said, drawing on his stockings and boots.\n",
      "By God and on my conscienc\n",
      "\n",
      "\n",
      "53.txt en en -13884.692095\n",
      "'Speak roughly to your little boy,\n",
      "By an incredible effort of will I even raised myself up, but only\n",
      "\n",
      "\n",
      "54.txt en en -71816.5253938\n",
      "I do not know whether Petronius is better than others of Cæsar's court, but he is different.\n",
      "And I c\n",
      "\n",
      "\n",
      "55.txt en en -99783.6966322\n",
      "I suppose you have heard of it; indeed, you must have seen it in the papers.\n",
      "\"We will sell our lives\n",
      "\n",
      "\n",
      "56.txt en en -23276.7380764\n",
      "As you come to the farmhouse from the front, you pass up a garden, with little enough in it, which l\n",
      "\n",
      "\n",
      "57.txt en en -93786.4902188\n",
      "Petronius looked at the slaves, among whom were beautiful and stately youths.\n",
      "Grete!\n",
      "Your character \n",
      "\n",
      "\n",
      "58.txt en en -46716.4681951\n",
      "His hair, which was short, sleek, and black, was just visible beneath the capacious brim of a low-cr\n",
      "\n",
      "\n",
      "59.txt en en -40182.6328778\n",
      "'He's a clever rascal,' he said between his teeth, his hands in his pockets.\n",
      "The old soldier had gro\n",
      "\n",
      "\n",
      "6.txt ca ca -51586.7470855\n",
      "Que es aixo?- zumzeja.\n",
      "Tot seguit digue:\n",
      "Tom! que passa, Tom?- I el sacseja i li mira el rostre, afa\n",
      "\n",
      "\n",
      "60.txt en en -85452.0652973\n",
      "Apres quelques instants, elle lui remit une petite bourse ou elle avait prepare quelques sequins.\n",
      "'I\n",
      "\n",
      "\n",
      "61.txt eo eo -16927.3833585\n",
      "Car sur ambau bordoj jen por vi patrujo.\n",
      "Eble la Grifo volos aldoni pluan klarigon.\"\n",
      "Mi tre baldau i\n",
      "\n",
      "\n",
      "62.txt eo eo -69238.7502244\n",
      "La festeno finigis: al la guf' li permesis La kuleron enposi; li mem ne forgesis La trancilon enpaki\n",
      "\n",
      "\n",
      "63.txt eo eo -18684.5012801\n",
      "\"Jen!\n",
      "Kiam fine cio retrankviligis, la kuiristino jam malaperis.\n",
      "Nu, au la sakto estis efektive tre \n",
      "\n",
      "\n",
      "64.txt eo eo -39310.0721203\n",
      "Kiam fine cio retrankviligis, la kuiristino jam malaperis.\n",
      "Kaj si trovis sin sola en ia longa malalt\n",
      "\n",
      "\n",
      "65.txt eo eo -77801.5627782\n",
      "Si sentis fortan ekscitecon kaj fortan deziron vidi la Damon.\n",
      "Nia plej satata volumo estis malgranda\n",
      "\n",
      "\n",
      "66.txt eo eo -29884.0572198\n",
      "Tra l' aer' vi sovas vin\n",
      "Supozis mi ke estis vi Malhelpo al la planoj (Dolor' vi for! ekkriis si) De\n",
      "\n",
      "\n",
      "67.txt eo eo -75259.1392035\n",
      "Si eligis krieton, krieton duone timan kaj duone koleran, penis forfrapi ilin kaj— —trovis sin kusan\n",
      "\n",
      "\n",
      "68.txt eo eo -12919.8150964\n",
      "Alicio komprenis ke la Kuniklo venas por serci Marianon.\n",
      "Staras vi sur la kap' kiel korpbalancilo?\n",
      "\"\n",
      "\n",
      "\n",
      "69.txt eo eo -34530.0508126\n",
      "\"Kaptu tiun Gliron,\" furioze kriis la Damo.\n",
      "Tamen post nelonge si sciigis ke si estas en la lageto f\n",
      "\n",
      "\n",
      "7.txt ca ca -23849.2346791\n",
      "-Be, els fotils son nostres, ara: oi?\n",
      "-Sona com… com porcs que remuguin.\n",
      "Que tenieu, Tom? -O jutge!\n",
      "\n",
      "\n",
      "\n",
      "70.txt eo eo -67950.7592154\n",
      "\"Kie la Dukino?\"\n",
      "Ekrigardeto tamen sur lian vizagon min konvinkis pri lia nepra sincereco.\n",
      "La Rego m\n",
      "\n",
      "\n",
      "71.txt eo eo -35015.0826513\n",
      "Tra l' aer' vi sovas vin\n",
      "\"Vi nur eldiru vian evidencon,\" ripetis kolere la Rego; \"se ne, mi nepre ek\n",
      "\n",
      "\n",
      "72.txt eo eo -38951.3126738\n",
      "Sed denove la infano gruntis.\n",
      "Alicio nenion respondis.\n",
      "\"Kian lingvon vi parolas?\" demandis la Agleto\n",
      "\n",
      "\n",
      "73.txt eo eo -12297.6718565\n",
      "La Agleto deklinis la kapon por kasi sian mokridon, sed kelkaj el la birdoj sibleridis audeble.\n",
      "Dum \n",
      "\n",
      "\n",
      "74.txt eo eo -45927.7509819\n",
      "Ho, ve!\n",
      "Vi ordonu ke si komencu.\"\n",
      "Si sentis sin ekdormanta, kaj jus komencis songi ke si marsas mano\n",
      "\n",
      "\n",
      "75.txt eo eo -70744.6655135\n",
      "Alicio demandis scivole.\n",
      "Subite si rimarkis antau si tripiedan tableton faritan el vitro, kaj jen! k\n",
      "\n",
      "\n",
      "76.txt es es -51969.4405555\n",
      "Habia cumplido veintisiete anos de cautiverio en esta isla, aunque deberia descontar los ultimos tre\n",
      "\n",
      "\n",
      "77.txt es es -27754.5809066\n",
      "¿Y quien hubiera podido disputarle la posesion de esa parcela submarina?\n",
      "Pero el duque es nuestro am\n",
      "\n",
      "\n",
      "78.txt es es -82716.609962\n",
      "Y luego, despues de un transcurso de dos minutos, se sacaria del coche lo que quedase de los dos cab\n",
      "\n",
      "\n",
      "79.txt es es -61526.9640761\n",
      "-La nota de Marianne, en que me decia que yo todavia le era tan querido como antes; que pese a las m\n",
      "\n",
      "\n",
      "8.txt ca ca -52513.3929454\n",
      "No es per a mi: no hi estic avesat.\n",
      "Pogue sentir com Mary plorava i de tant en tant introduia una pa\n",
      "\n",
      "\n",
      "80.txt es es -61165.7339303\n",
      "Quiza sea en virtud de tal razon por lo que duermen tranquilos en sus tumbas.\n",
      "–No, no estoy triste.\n",
      "\n",
      "\n",
      "\n",
      "81.txt es es -29787.8715136\n",
      "No se le ve nunca en ninguna parte.\n",
      "¿Acaso busco diversiones fuera de casa?\n",
      "-Aqui no ha de haber mas\n",
      "\n",
      "\n",
      "82.txt es es -6122.81591952\n",
      "-Entonces, la dama en cuestion, la senorita Grey creo que la llamo usted, ¿es muy rica?\n",
      "Estas dos pa\n",
      "\n",
      "\n",
      "83.txt es es -80710.9830294\n",
      "No se equivoca...\n",
      "Iba reventandolo poco a poco en sucesivas embestidas para las que tomaba impulso d\n",
      "\n",
      "\n",
      "84.txt es es -72099.9821495\n",
      "Todo lo cual se me representa a mi ahora en la memoria de manera que me esta diciendo, persuadiendo \n",
      "\n",
      "\n",
      "85.txt es es -26401.4190704\n",
      "Calculo su plan a la perfeccion.\n",
      "Mientras me preparaba para el duodecimo viaje, me di cuenta de que \n",
      "\n",
      "\n",
      "86.txt es es -56004.5046188\n",
      "Si, eso es sin duda dijo Milady, devolviendo la carta a la senora Bonacieux y dejando caer su cabeza\n",
      "\n",
      "\n",
      "87.txt es es -25757.6624072\n",
      "Yo no se, mi senor, como dar orden que nos vamos a Espana, ni Lela Marien me lo ha dicho, aunque yo \n",
      "\n",
      "\n",
      "88.txt es es -75879.4404822\n",
      "Ned y Conseil se sentaron en el divan.\n",
      "Alegrose algun tanto don Quijote, y dijo: -En verdad que esto\n",
      "\n",
      "\n",
      "89.txt es es -84786.6661058\n",
      "Con el tiempo, idee la forma de hacerlo, de la siguiente manera: Hice algunas vasijas de barro muy a\n",
      "\n",
      "\n",
      "9.txt ca ca -18385.0297218\n",
      "Per que no havien de fer-ho?\n",
      "He tingut batusses, pero mai amb armes.\n",
      "NOTA.-Les preteses composicions\n",
      "\n",
      "\n",
      "90.txt es es -17483.6616673\n",
      "Los Oblonsky habian quedado, pues, sin un centimo y sin tener donde encontrar dinero.\n",
      "Cada vez eran \n",
      "\n",
      "\n",
      "91.txt fi fi -76589.6790118\n",
      "Mina otaksun, etta ystavani selitys huomataan riittavaksi, ja kun huomenna palaan Lontooseen, on min\n",
      "\n",
      "\n",
      "92.txt fi fi -27611.5794207\n",
      "Nyt on maaraava hetki kasissa, Watson; me kuulemme askeleita portaissa, se on eras mies, joka kohta \n",
      "\n",
      "\n",
      "93.txt fi fi -8433.74680838\n",
      "En ole usein kuullut hanen nauravan, ja joka kerran on se ennustanut tuhoa jollekin.\n",
      "Mita teitte ikk\n",
      "\n",
      "\n",
      "94.txt fi fi -48416.5306201\n",
      "Kivien valiin painautuneina tarkastimme hopeareunaista sumumuuria edessamme.\n",
      "\"Ei, ei, sir; ei teita \n",
      "\n",
      "\n",
      "95.txt fi fi -47194.1311893\n",
      "Toiselta puolen aivan varmasti seudun koko koyha, puutetta karsiva vaesto on riippuvainen siita, ett\n",
      "\n",
      "\n",
      "96.txt fi fi -52633.0472092\n",
      "Mina haluaisin myoskin saada taman viiden punnan setelin vaihdetuksi.\"\n",
      "\"Ole valmis!\n",
      "\"Juoksenko heida\n",
      "\n",
      "\n",
      "97.txt fi fi -26638.3086639\n",
      "Muutamia viikkoja sitten siivosi han perin pohjin sir Charlesin tyohuoneen, joka oli ollut suljettun\n",
      "\n",
      "\n",
      "98.txt fi fi -30874.9413837\n",
      "Kuta jarjettomammalta ja hullummalta joku seikka nayttaa, sita huolellisemmin on se tutkittava, ja j\n",
      "\n",
      "\n",
      "99.txt fi fi -26979.5763582\n",
      "Grimpenin suo on ikava paikka.\"\n",
      "Sitten oli han mennyt edelleen pitkin kujaa, ja lahelta sen loppupaa\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testdirname = 'C:\\\\Users\\\\Maria\\\\Desktop\\\\data\\\\plain\\\\test'\n",
    "accuracy = language_model_uni(testdirname, ans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вау! Accuracy составляет 1. Поразительно, но если посмотреть на табличку сверху, то действительно оказывается, что данный классификатор безошибочно распознавал все документы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy = {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем разобраться с full. Для классификации данной выборки я создала модель, работающую на триграмах (3 символах). Посмотрим, как она справится с задачей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ru', 'fr', 'el', 'sv', 'it', 'nl', 'eo', 'pl', 'no', 'es', 'ca', 'de', 'en', 'fi', 'pt', 'hu'])\n"
     ]
    }
   ],
   "source": [
    "dirname = 'C:\\\\Users\\\\Maria\\\\Desktop\\\\data\\\\full\\\\train'\n",
    "language_model_tri = LanguageEstimator(dirname, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans_df_full = pd.DataFrame.from_csv(\"C:\\\\Users\\\\Maria\\\\Desktop\\\\data\\\\full\\\\ans.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt ca ca -25899.400384\n",
      "-Alla on caura aquesta sageta, enterreu-hi el pobre Robin Hood sota el bosc que treu ufana.\n",
      "-Gaire… \n",
      "\n",
      "\n",
      "10.txt ca ca -132580.167562\n",
      "-Allargueu els ricos al guanet de l'arbre mestre!\n",
      "Un pasme intrigat es desvetlla en tota cara de l'a\n",
      "\n",
      "\n",
      "100.txt fi fi -123255.401592\n",
      "Kuja johti suurehkolle ruohokentalle, ja talo oli edessamme.\n",
      "Times-lehtea lukevat harvoin muut kuin \n",
      "\n",
      "\n",
      "101.txt fi fi -99796.37974\n",
      "Meilla on kumminkin useita johtolankoja kasissamme, ja on hyvin luultavaa, etta joku niista vie totu\n",
      "\n",
      "\n",
      "102.txt fi fi -1124.89597816\n",
      "Tahtoisitko Bradleyn ohi kulkiessasi pyytaa hanta lahettamaan naulan vakevinta tupakkaansa tanne min\n",
      "\n",
      "\n",
      "103.txt fi fi -80203.8613288\n",
      "\"En joutanut edes panemaan hattua paahani.\n",
      "\"Kysymys on kai jostakin salametsastyksesta, luulisin\", s\n",
      "\n",
      "\n",
      "104.txt fi fi -12723.9091753\n",
      "Yhdessa kiiruhdimme alas portaita kadulle.\n",
      "Mina muistin tuon omituisen varotuksen, joka oli leikattu\n",
      "\n",
      "\n",
      "105.txt fi fi -141494.118059\n",
      "Ja tuon liekin luona nyt tama roisto loikoo odottamassa.\n",
      "Times-lehtea lukevat harvoin muut kuin hyvi\n",
      "\n",
      "\n",
      "106.txt fr fr -5423.77189432\n",
      "C'est qu'en effet, ce plomb etait sorti d'une arme a feu, et quel autre qu'un etre humain avait pu s\n",
      "\n",
      "\n",
      "107.txt fr fr -49800.911623\n",
      "Un pli amer apparut au coin des levres du visiteur.\n",
      "Milady comprit que Lord de Winter mentait et n'e\n",
      "\n",
      "\n",
      "108.txt fr fr -10305.8926822\n",
      "Ce procede acheva de desesperer Candide; il avait a la verite essuye des malheurs mille fois plus do\n",
      "\n",
      "\n",
      "109.txt fr fr -141085.087917\n",
      "Si j'avais tue du gibier au-dela, de ma consommation, il m'aurait fallu l'abandonner au chien ou aux\n",
      "\n",
      "\n",
      "11.txt ca ca -60233.2218717\n",
      "-Es tan bonic, sempre, una forada!\n",
      "Continuava la quietud solemnial.\n",
      "Aixi es que interiorment resolgu\n",
      "\n",
      "\n",
      "110.txt fr fr -15095.7900178\n",
      "Ses roues rapides, bruyantes, battant l'eau quiretombait en ecume, lui donnaient un air de hate, un \n",
      "\n",
      "\n",
      "111.txt fr fr -79737.1648603\n",
      "Laurent, d'un temperament plus epais, tout en cedant a ses terreurs et a ses desirs, entendait raiso\n",
      "\n",
      "\n",
      "112.txt fr fr -192866.438129\n",
      "—Si la soupe n'est pas prete, ca se comprend.\n",
      "Son menton etait appuye sur ses genoux, que ses deux b\n",
      "\n",
      "\n",
      "113.txt fr fr -66212.6313062\n",
      "– Il ne peut en etre autrement, procurateur, repondit Afranius sans rire, et meme d’un ton severe.\n",
      "L\n",
      "\n",
      "\n",
      "114.txt fr fr -166915.486863\n",
      "C’etait un vaste terrain inculte avec des buissons et des broussailles, une contree sterile, que Gle\n",
      "\n",
      "\n",
      "115.txt fr fr -81357.5205198\n",
      "Ou en etions-nous?\n",
      "--En ce moment elle evolue dans l'avant-port…\n",
      "He was in hopes of seeing Clelia ag\n",
      "\n",
      "\n",
      "116.txt fr fr -170455.68782\n",
      "Tout d’abord, il ramassa le passeport et le tendit a Maximilien Andreievitch, qui le prit d’une main\n",
      "\n",
      "\n",
      "117.txt fr fr -192829.605898\n",
      "Lorsque Alzire et les enfants furent la, elle partagea le vermicelle dans trois petites assiettes.\n",
      "–\n",
      "\n",
      "\n",
      "118.txt fr fr -94468.6221331\n",
      "-- Vous arrive-t-il souvent de vous promener avant le dejeuner, Mr Horscroft, demanda-t-elle, toujou\n",
      "\n",
      "\n",
      "119.txt fr fr -60600.3761931\n",
      "Non!\n",
      "Bon !...\n",
      "-- Etrange desir, madame Reed!\n",
      "Deja tout me parait moins facile.\n",
      "Elle lui conviendra t\n",
      "\n",
      "\n",
      "12.txt ca ca -167949.050273\n",
      "No va aconseguir un somriure ni una resposta: aixi es que recaigue en el silenci i deixa que son cor\n",
      "\n",
      "\n",
      "120.txt fr fr -80973.4627121\n",
      "La mer etait calme alors, et j'avais grande envie de m'aventurer dans ma pirogue jusqu'au navire.\n",
      "Or\n",
      "\n",
      "\n",
      "121.txt hu hu -162054.478342\n",
      "A jelen pillanatban is azt hajtogatta ugyan magaban, hogy a halal fia, de esze agaban sem volt szep \n",
      "\n",
      "\n",
      "122.txt hu hu -212499.724152\n",
      "- Hat, ha igazan ezt tartja - mondta Magnus ur -, akkor elarulok onnek egy kis titkot: en magam is r\n",
      "\n",
      "\n",
      "123.txt hu hu -137539.267465\n",
      "Szvijazsszkij asitozva kiserte ot ki az eloszobaba es csodalkozott azon a furcsa allapoton, a melybe\n",
      "\n",
      "\n",
      "124.txt hu hu -167304.159263\n",
      "A csudovszkij kolostorbol valok?\n",
      "Egy darabig tetovazott, aztan mas iranyba fordult s elindult vaktab\n",
      "\n",
      "\n",
      "125.txt hu hu -211441.659047\n",
      "Egyszeruen az uregen oly hosszu ideig atrobajlo aradat utolso nyomai voltak; a levego pedig enyhe ny\n",
      "\n",
      "\n",
      "126.txt hu hu -112096.249452\n",
      "Ne vedd rossz neven.\n",
      "Ezek allatborokbe varrt keresztenyek voltak.\n",
      "Fegyverek dontik el, vajon civiliz\n",
      "\n",
      "\n",
      "127.txt hu hu -206397.478706\n",
      "- Hamar, barataim! - kialtotta. - Elfaradt!\n",
      "Tom ebren fekudt, es nyugtalan turelmetlenseggel vart.\n",
      "E\n",
      "\n",
      "\n",
      "128.txt hu hu -213542.319418\n",
      "Ez a muszer mozgathato tukrok segitsegevel meghatarozza a nap es a latohatar altal bezart szoget, po\n",
      "\n",
      "\n",
      "129.txt hu hu -231850.531959\n",
      "- Csak lassan, lassan; igazan nincs mert olyan nagyon sietnie oda, ahonnet a legtobb ember kifele ki\n",
      "\n",
      "\n",
      "13.txt ca ca -23357.9785112\n",
      "Ho esborra, i s'enfurisma amb si mateix per la seva feblesa.\n",
      "Es una cosa de no-res.\n",
      "-Darrera els oms\n",
      "\n",
      "\n",
      "130.txt hu hu -139361.710118\n",
      "Bizony, megoregedett, - jegyezte meg a kamaras.\n",
      "Hiszek!...: Krisztusom!\n",
      "Panaszkodott, hogy rosszul m\n",
      "\n",
      "\n",
      "131.txt hu hu -122149.970732\n",
      "Fejem folott gomolygo felhok szalltak, optikai csalodast okozva: ugy ereztem, mintha mozdulatlanok v\n",
      "\n",
      "\n",
      "132.txt hu hu -3589.52793264\n",
      "Vad lo is megjuhaszodik,\n",
      "Haladok, lordom!\n",
      "Ha igy van, annyi aranyat adok Bussant Raonak, amennyi a s\n",
      "\n",
      "\n",
      "133.txt hu hu -160949.517414\n",
      "Ez mar dofi! - dormogte Pencroff.\n",
      "Szepen elrendezett, nagy kert volt ez, s Collins maga muvelte.\n",
      "\"Le\n",
      "\n",
      "\n",
      "134.txt hu hu -90707.1826147\n",
      "Valamennyi utas osszegyult a tarsalgoban.\n",
      "- Nincs megirva, hogy mindenki ugy vegezze az eletet... mi\n",
      "\n",
      "\n",
      "135.txt hu hu -36859.5622491\n",
      "Hacsak foglalatban nem akarta valaki latni oket, ennel tobbet valoban nem kivanhatott.\n",
      "A fiatal Scse\n",
      "\n",
      "\n",
      "136.txt it it -143052.696188\n",
      "Questo preparativo di legatura e la vergogna che per me ne derivava, calmarono la mia agitazione.\n",
      "Tu\n",
      "\n",
      "\n",
      "137.txt it it -124593.90781\n",
      "Bevvero un bicchierino per uno e tornarono alla tavola.\n",
      "Levin, accigliato, stava seduto, ascoltando \n",
      "\n",
      "\n",
      "138.txt it it -206434.094095\n",
      "Sara proprio una cosa straordinaria!\n",
      "Io voglio a questo proposito addurre uno esemplo moderno.\n",
      "Ella \n",
      "\n",
      "\n",
      "139.txt it it -58376.1398499\n",
      "— Dopo una giovinezza e una virilita trascorse ora in mezzo a inesprimibili sofferenze, ora in una d\n",
      "\n",
      "\n",
      "14.txt ca ca -146085.702483\n",
      "-Sst!\n",
      "Els minyons de dalt estaven tan excitats i tan delectats com ells mateixos.\n",
      "No ho voldria pas,\n",
      "\n",
      "\n",
      "140.txt it it -84841.531961\n",
      "I passi veloci scricchiolanti del servitore che camminava nel salotto lo fecero tornare in se.\n",
      "Il fa\n",
      "\n",
      "\n",
      "141.txt it it -12215.1885314\n",
      "Ma quelli che parlavano tacquero, e la sua domanda sconveniente fu udita.\n",
      "Trovai allora il vascello \n",
      "\n",
      "\n",
      "142.txt it it -37801.7835929\n",
      "Fui trasportata in sogno nella mia fanciullezza; mi pareva di trovarmi nella camera rossa di Gateshe\n",
      "\n",
      "\n",
      "143.txt it it -1538.97825801\n",
      "Ma nel piccolo ingresso si fermo per riflettere a quello che era accaduto.\n",
      "A mezzanotte torno gelato\n",
      "\n",
      "\n",
      "144.txt it it -153454.970948\n",
      "Dopo cio tornai alla mia fortezza, ove mi posi a lavorare per lui; e prima di tutto gli diedi un pai\n",
      "\n",
      "\n",
      "145.txt it it -204148.654203\n",
      "Ma cosa mai!\n",
      "— E unita anche cosi — ella rispose in modo appena percettibile.\n",
      "L’ha detto del resto: \n",
      "\n",
      "\n",
      "146.txt it it -32571.1009392\n",
      "I duri mattoni rossi non si son che consolidati col tempo, e le loro scale di quercia non scricchiol\n",
      "\n",
      "\n",
      "147.txt it it -80887.06541\n",
      "Ella lo guardo diritto nel viso, quasi supplicandolo di farle grazia, e gli porse la mano.\n",
      "Vidi dise\n",
      "\n",
      "\n",
      "148.txt it it -224114.853961\n",
      "Liberata da quell'oppressione, stabilii di lavorare con nuova lena e di aprirmi la via a traverso og\n",
      "\n",
      "\n",
      "149.txt it it -208051.321745\n",
      "Io che mi vidi ridotto al mero stato di natura, io la capii con mio giornaliero scoraggiamento quest\n",
      "\n",
      "\n",
      "15.txt ca ca -63720.0191525\n",
      "Aixi es com ho fan els nens i les nenes bons minyons.\n",
      "Hi hague silenci per algun temps.\n",
      "-Molt be, Hu\n",
      "\n",
      "\n",
      "150.txt it it -214331.153989\n",
      "Tre cose m’incoraggiavano: primieramente un dolce placido mare, in secondo luogo la marea che saliva\n",
      "\n",
      "\n",
      "151.txt nl nl -31824.3542457\n",
      "Het was het onbrandbare hout, dat Paganel had opgenoemd onder de zonderlinge voortbrengselen van Aus\n",
      "\n",
      "\n",
      "152.txt"
     ]
    }
   ],
   "source": [
    "accuracy_full = language_model_tri(testdirname, ans_df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, точность здесь уже оказывается не 100%, наш классификатор допускает ошибки. Однако, все равно процент правильно определенных текстов очень высокий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy (for full test) = {}'.format(accuracy_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
